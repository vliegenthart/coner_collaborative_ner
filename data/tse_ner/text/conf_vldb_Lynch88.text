Introduction
 While current query optimizers such as those in System R/DB2 
The empirical data in this paper is taken from a large bibliographic database used by the MELVYL@ online union catalog, a replacement for a traditional library card catalog that provides access to the holdiugs of the libraries of the University of California ninecampus system. The MELVYL catalog is described in detail in 
The value for n in the Zipf distribution model for each of the keyterm indices can be determined by observing that CARD(I) = UNIQUE(I) uaverage En, number of occurrences of a value in or CARD(Z)/UNIQUE(I) = n/ H,. For subject keyterms n/ H,, = 150, so n is 1143; for title keyterms n/Hn = 20, yielding n = 105. It is clear from these figures that, as is common with Zipf distributions used to model bibliographic databases, the match between the theoretical distribution and the empirical data is not close for the most frequently occurring terms. The most common terms in the actual title file, which contains about 950,secondary indices exist for all predicates involved iu bhe query (which will be typical in a bibliographic retrieval environment), the query planner will choose the predicate it believes to be most selective (i.e., satisfied by the smallest number of tuples), use the index for that predicate to obtain the TIDs of the tuples satisfying the predicate, and then read these tuples and verify that they sn.tisfy the remaining predicates in the conjunctive query. A current query planner will typically estimate se- lectivity for an equality predicate by assuming a uniform distribution of column values when an index is available, or statistics on the number of unique values appearing in a table for a given column are maintained . Assuming a column C is indexed by an index I, selectivity of a predicate such as (C = due) will be estimated by l/ UNZQlJE(Z). For set-valued relations, where there can be more than one entry in the index for a single column value appearing in the relation, this formula must be generalized to estimate selectivity of the IN operator to CARD(I)/CARD(T) * UNIQUE(I). This uniform-distribution-based estimate gives an expected selectivity of l/200,000 for title keyterms and 3/80,000 for subject keyterms. The key point about uniform distribution selectivity estimation is that sclectivity is estimated independently of the constant that appears in the predicate. In the first query, System R or 1NGR.E will always choose the predicate involving t.he title keytcrm since the selectivity of values in the title keyterm index is much smaller than the selectivity of values in the subject keyterm index, and will use the title predicate to select rows to read and exir.mine. This may or may not be a good decision, depending OJI the values of term1 and term2; in Jllilll)' c'ases a randomly selected subject keyterm will be more selective than a randomly selected tit,le keyterm. Specifically, if the subject keyterms are Zipf(m) and the t,itle keyterms are Zipf(n), the probability that using the title predicate to select the ac- H (1) n- H cess path is correct is = 1 + w ,nH ~ 2H,. where H :' = C;= " =, l/i " , th c n tl I grnera.lizcd harmonic number . (Details of this and other computations are omitted here in the interest of space; see 
For m = 1143 and n = 105, this probability is .6768; thus, the selected pln.n is correct about two out of three times. 
In the second example query, the optimizer cannot differentiate between the selectivity of the first and second predicates involving title keyterms; it will ar- bitrarily select one. The likelihood that it will choose the more selective predicate is somewhat better than .5. (It is better than .5 because the two terms may have equal selectivity.) Assuming the distribution of keyterms modeled by Zipf(n), a similar computations 
 shows that the correct choice will be mde with probability l/ 2 + Hi2'/ 2H2,. 
For n -= 105 (the value for title keyterms), the probability that the correct plan is chosen is .5298. For n = 1143 (a query similar to the second example but involving $ubject keyterms rather than title keyterms), the probability that the planner selects the correct plan is .5142. The reason the probability is better than .5 is that some l/H, of the subject keyterms only occur once; no matter which predicate is selected for the access path, the selection will always be correct at least that often (13% of the time for n = 1143). As queries become more complex, the likelihood that the planner will choose the optimal plan drops off quickly. Similar computations show that for n = 105, a five-term query only will be evaluated optimally with probability about 
.27. 
Thus, t)he planner oft,cn v. ill select the wrong plan. This erroneous choice may not he disastrous if t.he select .ed plan is only slightly suboptimaL If the sclect,ed pl<m requires an order of mngnibude more I/O thnu t,he optimal plan, however, the error is quite serious. To measure the effects uf bad plan selection due to iucorrrct , selectivity estinmtion, the avcrnge uu*nber of I/O oper:lt,inns rcquirrtl for queries in the form SELECT * FROld BOOKS y!HERE terml IN TITLE-KEY- TERMS AND term.2 III SUBJECT-KEYTER!dS; will be conlput.ed over all pa.irs (terml, term!?) where term1 is a title keyterm and termZ?is a :.llbject kcyterm. Query plans will be selected both by using the System R/D132 Ig tl a ori 1111 and uuder the nssumpt.ion that the DBMS has perfect knowledge of predicate selectivity (and, thus, always selects the optima1 plan from the set of plans considered by System R/DB2 in tilis situation). We assume that the values of TITLE-KEYTERMS are Zipf(n) and that those of SUBJECT-KEYTERMS are %ipf(m). 
The average number of I/O operations required for plans selected by the System R/DB2 planner is simply 5 i/iH, = n/ H,, since when a term that oc- i=l curs i times is used as the access path, i ren.ds are uecessary t,o obt.ain the corresponding tuples. 111 contrast , if the planner always selects the optimized plan, t.hen the n.vcrage number of I/O operations per query 
is -= l/ ff,,ff,,& ( (H,,
, + 2) " -(n + 1) H,,) . For the special case m = n, this simplifies to (2n --H,,)/ H,. 
The standard System R/DB2 plms require n/H, 
- Pn Hn)/ Hi = ((n + 1) H, -2n)/ HE additional I/O operations, on average. 
For the first example query, the expected number of I/OS for a plan selected by System R/DB2 is 20.057. Always choosing the optima1 plan would produce an average of Il.4054 I/OS. For the second example, System R/DB2 requires 20.0537 I/OS on average, again, while the average is 7.4691 for optimal plans. On a query ana.logous to the second exn.mple, but involving subject rather than title keyterms, System R/DB2 requires an average of 150.0184 I/OS, while the optimal plan only needs 39.2485 I/OS on average. The expected cost for the plan selected by DB2 diverges even more rapidly from the expected cost for the optimal plan for queries involving more than two terms. For example, with three conjuncts and n -105, the plan selected by DB2 still requires 20.0537 I/OS on average, while the optimal plan requires only 3.9873 I/OS. For three conjuncts and n = 200, DB2 plans average 34.0250 I/OS, while optimal plans avcrnge 5.5626 I/OS. 
Selectivity Estirmtion
to occur less frcqueutly or that a v:llue tlistribution is rclntivcly 1111if0rm whcJ1 soJiie group of excc'ptiuna .1 values is rc~~ioved. Exnl.:ples of the use of nl)plicnt,ioll-speciTIc kuowlcdge i ,i cstinl.4!1ion will he tlevelupcd ill Section 5. 0 Sclcctivity eslmiators are use d to est,imate the se- lectivity of predicates appearing in queries. The stn&tics that c.linracPerize the selectivity of such p4icates may he dilfcrent, from the statistics that characterize a database. Given a large enough sn.mpling uf queries, it may be wort.hwhilc to adjust I.he xrleclivit,y estimators to reflect l.hc biases of users qucryilrg t,he database. With some bibliogr ;iphic tl;il abases (particula.rly when users receive training bcforc using them), users t,end t.o ;lWJid t,he most commonly occurring keyt,erms because t,hcy have lit,tle retrieval precision ant1 produce uuwieldy query results. Sirniln.rly, a substantial number of predicat,, s of the form (colu7nn = value) actually fail to match any tuples in a public access information ret.rieval system (often because of a high incidence of keyboarding and spelling errors when queries 'are entered and a lack of understanding of indexing pra.ctice by retrieval syst,em users). The effects of these phenomena can only be reflected in selectivity estimation by a user-defined estimator procedure. 
Definition Registry and Choice of
Predicate Selectivity Estimators A predicate selectivity estimator is a procedure (either built-in or user-defined) that returns an integer giving t.he estimated number of tuples that will satisfy the predicate, and has as input parameters the predicate being estimated and st.atistical data JllailltaiJled by the DBMS about the t,able referenced in the predicn.te and any associated indices. A predicate selectivity estimator procedure is registered with the DBMS through the directive DEFINE PREDICATE ESTIMATOR procedure-name 
By definition, all predicate estimators take the same input parameters and return the same result parameters . Thus, there is no need to enumerate the parameter datatypes in the registration directive. A registered estimator is attached to a user-defined Booleanvalued binary operator, operator-name, through the ESTIMATOR parameter that is specified by extending t,he syntax of the DEFINE OPERATOR direc- tive 
 Locnl est,iJJJators caJ1 be defined that take precedence over the global default cst,imator when predicates involve specific tables or specific colu~JlJis of specific tables . Such locn.1 cstimn.bors are defined by associa.ting a regist,crcd estimator with au operator using one of the The first directive indicates that the specified estimator is to be used to estimate selectivity of predicates in the form (column relop value) where column is any column in the specified table. The second form of the directive defiJles the estilnator to apply to predicates of the form (column-name relop value). The query planner always uses the JJiost specific estimator ava.ilable to compute the selectivity of a predicate. 
Estimators for Predicates of the Form (value IN column) in Bibliographic Retrieval Systems
This section describes the construction of an extremely accurate estimator for predica.tes of the form (term IN TITLE-KEYTERMS) as au example of the applicability of user-defined predicate selectivity estimators. The statistics of title keyterms in the MELVYL-database are typical of many bibliographic databases, and a similar a7.nalysis and approach can be used to develop es- timators for other predicate types such as (term IN SUBJECT-KEYTERMS). Section 5.1 discusses criteria used to measure the quality of estimators. Section 5.2 briefly reviews prior work in both parametric and nonparametric estimators for selectivity and discusses why these prior approaches fail to meet the needs of bibliographic retrieval systems. Section 5.3 describes methods for actually constructing estimators. Finally, Section 5.4 uses the criteria described in Section 5.1 to evaluate the performance of the estimators developed using the proposed methods against other approaches discussed in Section 5.2. s If the average is used as the estimator, then the RMS crrur is cqun.1 t,o the standard dcvia.tion. The normalixecl RMS error, which measures t.he relative error of an cst,inln.tor, is written NRMS(I(:, f') and defined as SEL,q(p))2 cARlI I'EP ,, l --1 ---spI (p) " 
.-' 
 We will be concerned with two main sets of prcdicates: the set D of all predicates of t(he form (v&e IN c0lum.n) where vulue appears in column for some row of the table, and a set Q of sample predica.tes taken from queries. Note that Q can contain predicates t.hat do not match a.ny tuples in the datab'ase, while all predicates in D match at le,ast one tuple. The subset Q' of Q consisting of those predicates in Q that ma.tch at least one tuple in the database will also be used. 
For the experimental resulbs given here, the set Q cont.ains 817,093 title keyterms t#hat were extracted from a sample of 885,930 MELVYL catalog FIND commands (of which 326,511 referenced bhe title keyterm index) recorded from public access MELVYL catalog termino.ls during part of 1986. These 817,093 keyterms were extra.ctcd from a total of 1,017,306 title keyterms nppen.ring in the FlND commands; t.he remaining title keyt.erms were discarded because they were stoplisted words not indexed in t.he MELVYL data.base, they contained invalid chara.cters, or they nc.tun.lly were partial match specifications for title keyterms. The set D consists of the 951,008 different title keyterms that appeared in the MELVYL database as of December 12, 1986. This set was actually derived from a larger set of 954,531 terms, some of which cannot appear in user queries (because they have been stoplisted but were partially indexed in the database prior to stoplisting, or because they contain chnrncters t,hat ca.nnot he entered by the user in The first parametric approach to selectivity estimn.tion was formalized in 
More generalized parametric approaches were proposed by 
The literature contains no co:!sideration of appropriate mappings, although there has been some work done on appropriate dist r.ihut,ions to describe bibliographic dnt.nbnses, which a:'sumes that values appearing in an index have been mapped to an integer interval (typica .lly through frequency ra.nking). Unfortunately, frequency ra.nking amounts to a map from strings to integers d&nod Ly an explicit table. The storage for such a table will be nearly as large n.s the iudcx itself, rendering it useless for most estimation processes. Certainly, thrrc is no map honk keyt.erms in n.lphabebical order, for example, t,o rank frcqucncy, which C~JI be determined by simple interpolation or other nont.nbular ~nethods. 
Nonparametric Estimation 
IK ooi 1980, Piatetsky-Shapiro, & Conuell 1984, C'hristodoulakis 19811 propose variations on a nonparametric estimabion met,hod ca.lled histograms. While their work seems to have been w&en with l.he estimation of selectivity for numeric keys in mind, it is easily extended to character-valued keys. The basic idea of hist.ogrnm estimation is that the ra.nge of key values is pa.rtit.ioned into a set of subranges RI, Ra,. . .,R,. This can be done either a.lgorithmically (for example, by dividing the range from t! to h into fixed-length segments in the case of a numeric index), or by a table of explicit subrange demarcation points determined according to some criteria. Each subrange Ri is then modeled using some distribution Di. All three papers propose use of the uniform distriLution, but a.n a.rbitrary distribution could be used. 
The major difficulty with the currently proposed nonparametric methods for a bibliographic database is that they assume that ranges of keyterms in some natural ordering (such as collat#ing sequence) have similar distributions, or at lea.st that using a uniform a.pproximation for the selectivity of moderate-sized groups of keyterms will be sufficient. The assumption of local %moothness " in the distribution does not hold true in practice. when terms are listed in collating sequence, selectivity varies wildly from term to term. 
Construction of an Accurate Selectivity Estimator for Title Keyterms Construction of a selectivity estimation procedure for title keyterms is approached by piecewise approximation . First, frequently occurring keyterms are identified: their selectivity is estimated precisely by maintaining an actual list of these keyterms and the number of tuples they select within the estimator procedure . Subsection 5.3.1 gives both t,heoretical and cx- perimental n.ualyses of the tratle-off between mt*luory requirements and est,imator accuracy. For the remaining keyterms (which a.rc uot itlentified a.s very commonplace), the overall approach is to pnrt.ition the set. of possible keyterms into class;cs, based on some application-dependent criteria, which GIJI be expected to col,lolJte with selectivity, and ~.IIcJ~ t.o cnlculatc a uniform approximation for keyterm selectivity within each class. The estimator tont.nins lists giving the avera.ge vnhle to be used for each class. Once a class is assigned to 311 iuput keyterm, the selectivity estimate to be used is looked up in the list. The psrtitioning mechaui?rns that are considered in Subsection 5.3.2 are kcytcrm lenglh (the uumber of characters in the keyterm) and analysis of digrams (n.djaceut letter pairs) that occur in kcyterms. Keyterm length GIJI be expected to correlate to selectivity in that long words are used less often than short words; there is a natural tendency in the development of language towards abbrevin.ting long words t,hat are fr( quently used, or supplanting them with shorter sy~lo~~y~ns. Digrarn frcquency is a well. known " signature " of romance lmguages and has been used for centuries in cryptn.nalysis [Kahn 13671. It is rensonable to expect that a word contn.ining an infrequently used digram will not be used often. Analysis of selectivity for those keyterms for which selectivity must be heuristically estimated (since t,hey are not explicitly listed in the estimator procedure as are frequently occurring keyterms) reveals an interesting and somewhat unexpected phenomenon. The selectivity statistics for keyterms zlsed in queries are radically different than the statistics obta.ined by considering all keyterms that occur in the da.tabase. This application-specific knowledge is also incorporated into the estimation algorithms of SubsecGon 5.3.2 by assigning the selectivity estimate con&ant for each keyterm class based on query statistics rather than database statistics. If we ;tisume that m kcytcrms are listed explicitly in the estimator along with their selectivity, several approa .ches cau be taken for estimating the selectivity of input keyterms that do not appear in the list of m keyterms. The simplest approaches use uniform ap- proximation for the remaining keyterms. The more elaborate (and hopefully more accurate) methods sttempt to partition the remaining keyterms into keyterm classes so that the select.ivity for members of ea.41 keyterm class can be well-approximated by a constant estimate over that keyterm class. This section defines a number of approaches a.nd in some cases provides empirical data supporting the use of the approach. Subsection 5.4 actually compares the performance of the estimators defined here. 
Selectivity
Uniform Estimators 
A Uniform(m, S) estimator employs a list of the m terms that appear most frequently in the database and their selectivity; a selectivity estimate for an input keyterm that appears in the list is taken from the list. For keyterms tl1a.t do not appear on the list, se- lectivity is estimated by averaging the selectivities of all the keyterms that appear in the set S but are not among the mkeyterms explicitly listed in the estimator. Note that Selinger's uniform approximation is simply Uniform(O,D). 
There is a significant correlation between length and selectivity; in addition, the average selectivity of keyterms appearing in user queries of a given length is quite different from the same list when computed using terms appearing in the database. 
Digram Estimators 
A Digram(m, S) estimator again employs a list of the m most frequently occurring terms in the database and their selectivity. Digram estimators use two additional lists. The first list contains all two-letter pn.irs occurring in database keyterms and the frequency with which each two-letter pair occurs. The second list is of all digrams from terms in S (except for those in the list of the m most common terms) and a selectivity estimate developed by averaging the selectivity of all keyterms in S that have the specified digram as the least frequently occurring digram in the keyterm (based on the occurrence frequencies for digrams in the database given in the first additional list). 
More precisely, for any digram 6, let FREQ(6 ) ;= CARD({w E Dj w contains the digram 6 }) For any keyterm w, LFD(w), tl le 1 east frequently occurring digram in w, is defined as LFD(w) = 6, where 6 is a digram in w and FREQ(6 ) 5 FREQ(7) for any other digram 7 in w. For any digram 6, let S(6) = AVG(SEL(w)), h w ere this average is taken over the set {wlw E S -{explicitly listed keyterms} and LFD(w) = 6 }. For any input keyterm not on the list of m most common keyterms, S(LFD(w)) is used as a selectivity estimate. 
There is a significant correlation between the value of a word's least common digram and that word's selec-' tivity. Again, there is a. subs:tal*tial variation bcl.ween I he &imated sclcct,ivity derived from the sets Q n.nd D, as \&I1 bhe lcugth estimators. Due to t!lc size of t,hc lists involved, howcvcr, they will not be reproduced here. 
Minimum Vnrinnce Estimulors 
A Miuva.r(,, S) estimator combines the lengt,h aad digra .m approaches. It employs a length estimator and a digram cst,imnt,or for input keyt,erms not on the list of m common kcyterms. However, it also includes lists of variances for the estimates provided by the lengt,h nnd digram est,imators nud s;clc& the cstimntc lvith the lower vn.rinnce for each input keyt.erm not explicitly listed wit.1~ its selectivity in the list of the m most common keyterms. 
Iktogram Estimators 
A lIistogrnm(lc) t es imator is developed by choosing every /c th kcyterm (t;) from a list of all keyterms in D in alphabetical order, and associating with each one of these chosen keyterms a selectivity estimate developed by avern.ging the selectivity of t.he k -1 keyterms in D immediately precceding the selected keyterm, plus the selectivity of the chosen kcyterm t; it.self. If an input keyterm w falls into the sequence of chosen keyterms on the list as t ,' < w < t ,,+,, then the estimated select ,ivit,y value for t ,L+i is ret.urned by the estimator. 
Based on the discussion in 5.2, these estimators should not be expect.ed to perform well in the bibliographic retrieval environment. However, since they are used by INGRES as a means of improving on the selectivity estimates of System R/DBP, their performance will be examined in comparison t.o the other estimators defined in this section. 
A Comparison of Estimator Performance
Table 2provides RMS and normalized RMS values for the various estimators when applied to three predicate sets: D (all keyterms in the database), Q (all keyterms in user queries), and Q' (all keyterms in user queries that match keyterms in the database). For t,hose estimators that use variable amourits of memory (all those except for the Uniform(0, S) estimator included for comparative purposes since System R/DBZ uses it), two memory sizes were used: about 5,000 terms in memory (requiring about 68KB) and about 25,000 terms in memory (requiring about 318KB). While the estimators are listed as 5,000 and 25,000, actual values varied slightly because t.he cutoff for storage in memory was by sclcctivity value, a.nd multiple keytcrms existed with the desired selectivity dues. Consequently, for example, all of the est,imators except for the histogram cst.inintors really used a v;llue of 4,333 r&her t,llan 5,000, and of 24,847 rather than 25,000. A uumber of conclusions can he drawn immedint ,ely from '
 There is not a great deal of difference in the performa .nce of Length, Digram, and Minvar estimators for a given amount of,,memory. On the basis of simplicity, Length estimators thus seem to be the best choice. , 
The poor performance of the digram estimators is somewhat suprising since the digram estimators intrinsically should contain more information than length es- timators. It may be necessary to use n-grams with n _> 3 to obtain more precise estimates, or to apply digram estimation in more elaborate ways. To gain some sense of how the various estimators would perform on keyterms appearing in actual user queries, comparisons for the estimators were also run on several randomly selected subsets of Q. The results of these comparisons are similar to those in 
Conclusions 
 This paper has shown that current query pla.nners often fail to select optimal plans even for simple queries in an environment where column values have highly skewed distributions. These errors result from the inability of current selectivity estimation methods to cope with highly skewed selectivity distributions and are very costly. To solve this problem, a general mechanism for incorporating user-defined selectivity estimation into an RDBMS has been proposed. New estimation techniques suitable for use with textual or bibliographic databases having highly skewed attribute selectivity were defined and compared to existing methods. It seems clear that the estimation techniques described here are useful in a bibliographic or textual database environment. It would be interesting to compare these estimation methods to more sta.ndard sel4vity estimation methods on text-oriented but nonbibliographic files, such n.s those found in business applications. If user-defined scleclivity estimation is to be iucorporated in extensible database systems, it will be necessary to develop tools n.ud theory to a.ssist in the creation of appropriate estimat(ors for various types of databases. This paper takes a first step toward this goal by dcfining a set of performance measures for estimators, as well a.s culargiug the repertoire of available estimation met.hods. 
The user-dcfincd estimator scheme described here is actually a somewhat simplified version of [Lynch 19871, which also provides user-defined estimators for AND and OR operators bet,ween predicates, and extends the predica.te selectivity estima.tor definition to accommodate LIKE predicates. While techniques similar to those presented here can be used to estimate selectivity of LIKE predicates fairly effectively, estimation for AND and OR operators requires radica.lly different techniques and seems to be a ~nucll more difficult problem. The experiments described in 
The results presented in 
