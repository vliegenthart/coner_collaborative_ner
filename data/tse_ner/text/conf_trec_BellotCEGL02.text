Introduction
Our approach combines a Named Entity Recognition System developed at Sinequa 1 and an answer retrieval system based on Vector Space model that uses some Knowledge Bases developed at the Laboratoire d'Informatique d'Avignon 2 . First, the Named Entity Recognition system is briefly described, including specific features (section 2). Then, a summarized description of the SIAC (Segmentation et Indexation Automatique de Corpus) information retrieval system is given (section 3). For the purpose of Question Analysis, several Question Taggings have been employed, they are exposed in section 4. The approach using Knowledge Bases is then depicted (section 5), with a summary of its coverage (section 5.3). Section 6 is devoted to the Question Ordering problem. Finally, we present several experiments in the frame of TREC-11 (section 7). 
Named Entities
Detection of Named Entities (NE) is one of the key elements in the Question Answering task. In the past few years, there was a growing interest in NE analysis. Most current techniques for NE recognition are based on handcrafted finite state patterns 
With regard to the output, XML has been used to represent the tagged documents, as shown below: 
"<NPP>Brown</NPP>, <PROF>director</PROF> of the <ORGAN><CITY>Los Angeles</CITY> Centers for Alcohol and Drug Abuse</ORGAN>." 
One can observe from the previous example that embedded entities are allowed. 
Normalization Function
The identification of all the occurrences of person names is a difficult task when performed by transducers only. Many reasons could be mentioned to explain this phenomenon. The most common case is when a LAST NAME is given without any FIRST NAME. We are also aware that our resource of FIRST NAME is not (and will never be) exhaustive. This prevents us from using this semantic information in order to detect person names. However, as observed by several authors, in most cases, person names are given at least once in full form (FIRST NAME followed by eventually a MIDDLE NAME and the LAST NAME). This appears to be exact when dealing with newspaper articles (their style obeying certain editorial rules). In order to reduce the number of unrecognized person name occurrences, a straightforward algorithm was developed based on the previous observations. First, the LAST NAME parts of the detected person name are extracted. Then, the document is parsed again in order to detect all the LAST NAME occurrences that were forgotten by the transducer. This could be done thanks to the person name previously extracted. The additional person name could then be used by the other transducers in the sequence. 
In the following example, a correct normalization of the person's name "John Paloma" is presented: The biggest inconvenient of this technique is that an incorrect detection of a word as a last name will affect the rest of the document processing. This mainly occurs when a first name is ambiguous (e.g. Rose, France, …). 
Pronominal Anaphora Resolution
Pronominal Anaphora is the most widespread type of anaphora. Resolving them could lead to an improvement in the Q&A task. For example, the following question expects an answer of type DATE: 
"When did president Herbert Hoover die ?" 
One of the top documents found on the Internet contains the answer to that question. However, the sentence containing the answer does not contain the key element "Hoover", but only the anaphora "he": "After his 1932 defeat, Hoover returned to private business. … 
He died in New York City on October 20, 1964." 
Resolving this particular case would greatly help finding the correct answer. For this reason, we have chosen to develop a Pronominal Anaphora Resolution, even though it is a " naïve" one. We decided to not resolve all the pronominal anaphora, but only for personal pronouns he and she, when they do not occur in quotations. Although this method is quite naïve, we achieved reasonable results on our test corpus. However, we have not yet evaluated the benefit of such a resolution in the whole QA task. 
SIAC
The SIAC information retrieval system (
∑ ∑ ∑ ∈ ∈ ∩ ∈ = S u Q u Q u S u Q S u Q u S u w w w w Q S . . . ) , cosine( 2 , 2 , , , (1) 
with: for document words: w u,S = TF (u, S). 
1 −log 2 N (u) N       (2) 
for query words: 
w u,Q = TF ( u,Q ). 1 − log 2 N ( u ) N       (3) 
Question Tagging
We defined a hierarchical set of tags corresponding to the types of expected answers (see appendix 10-1). This set was built according to a manual analysis of the TREC-9 and TREC-10 Q&A questions. For tagging TREC-11 Q&A questions, we have developed a rule-based tagger and we have employed a probabilistic tagger based on supervised decision trees 
Rule-based tagger
Our rule-based tagger is a set of Perl scripts. The main input consists on an XML file that contains 156 manually built regular expressions. These regular expressions are not exhaustive since they are based on TREC-9 and TREC-10 questions only. The following is an extract of this file: the <CITY> tag defines 3 question patterns for which the expected answer is a city. 
<CITY> <s> ZTRM <\/s> (In IN in )?((
Probabilistic Tagger
The probabilistic tagger is based on the named-entity recognizer presented during ACL-2000 
To " grow " decision trees, one needs a sample corpus (manually tagged TREC-10 questions in our case) and a set of key features to split tree nodes. The list of features is generated from the training corpus. Each feature corresponds to a sequence of words and/or POS tags. Splitting is made by asking whether a selected feature matches a certain regular expression involving words, POS and gaps occurring in the TREC-11 question. 
In order to evaluate our probabilistic tagger, we have subdivided the 500 TREC-10 questions into two sets: a learning set (259 questions) and a test set (150 questions). Over this 150 questions test set, we obtained a 68.5% precision level for 127 questions (23 questions were not tagged because the probability of the chosen tag was less than a minimal threshold). For example, CITY is the tag chosen for question 1204 whereas all other candidate tags have a zero probability. 
Question 1204: 
sample_1204 <s> ZTRM </s> What WP What is VBZ be the DT the cap=tal NN capital of IN of <UNK> NP <UNK> ? ZTRM ? </s> ZTRM=</s> = CITY sample_1204 ACTOR_ACTRESS 0 BIOGRAPHY 0 BIRD 0 BODY_PART 0 CITY=1 COMMON_WORD 0 COMPANY 0 CONTINENT 0 COUNTRY 0 COUNTY 0<=R> CURRENCY 0 DATE 0 DEFINITION 0 DEPTH 0 DIAMETER 0 DISTANCE 0 DURATION 0 EVENT 0 EXPANDED_ACRONYM 0 EXPLANATION 0 EXPLORATOR_RESEARCHER 0 FAMOUS_NPP 0 FAMOUS_PLACE 0 FAMOUS_PLACES 0 F=OWER 0 FOOD 0 HEIGHT 0 HEMISPHERE 0 INVENTOR 0 LENGTH 0 <=R> MEDIA 0 MINERAL 0 MONEY 0 MOUNTAIN 0 MUSICIAN 0 NUMBER 0 =THER_NP 0 PERCENTAGE 0 PHRASE 0 PLANET 0 POLITICIAN 0 POPULATION 0 RIVER 0 SEA 0 SEASON 0 SPEED 0 SPORTSMAN 0 STAR 0=00 STATE 0 TEAM 0 TEMPERATURE 0 UNIV 0 VEGETAL 0 WEIGHT 0.0<=R> 0 WRITER 0 YEAR 0 
In order to tag TREC-11 questions that were not tagged by our rule-based tagger, the learning was realized over the whole set of TREC-10 questions. 
Filtering and Answer Extraction
The sentences allowing to answer questions do not necessarily contain a word of the questions. At the opposite, a sentence may contain some keywords of the question without being related to it. Thus, a classical retrieval scheme such as similarity computation in the vector space model is not sufficient. In our case, the sentences from top-docs (the list of topdocs is the one given by NIST) are ranked by SIAC according to the similarity between them and the question. We had no time to implement a specific module to detect the focus of questions or to analyze their domaindependent semantic properties. In order to filter sentences that probably did not contain the answer, we only kept those with a proper name appearing in the question 4 and those containing an entity of the same type than the expected answer type. This strategy prevents us from answering some questions (a NIL answer is given by the Q&A system because of the lack of proper names in the ranked sentences and/or in the question) but it enables us to select some answers more easily. 
The Use of Knowledge Bases
We have chosen to take benefit from a set of knowledge DataBases (KDB) for several reasons, mainly: i.) Assess the reliability of our search engine, ii.) For a given relation between two NE, provide a bootstrap that may be used in the later steps of an iterative process (we plan to develop it soon). This process will be useful to extract other instances of such relations from full text collections. Therefore, it may be misleading to consider that the underlying idea of this component was to constitute a large Data Base of FAQ (Frequently Asked Questions), even though it has also been used as such. 
Coupling SIAC and the use of KDB
The link between a question and the production of the KDB component may be seen as a relation more than a function since the output may be multiple. To handle this (1-n) generation, we found it convenient to code the set of candidate answers using a regular expression. This regular expression is then applied on the sentences extracted by the search engine for 2 purposes: i.) Select the most likely answer ii.) Provide a support to the answer as required by the QA TREC protocol. 
Some characteristics of the KDB used
USA topics
As it appears obviously from a quick analysis of the Q set (TREC-8 through TREC-11), several questions are focused on various attributes related to the United States of America. Thus, we have searched the net (mainly from the following url: http://www.50states.com/ ) in order to collect as many data related to these topics as possible. The coverage of such a " USA-centered " KDB is shown in 
Book topics
In another direction, we have included in this process the relation book/author (who wrote the book " title " ?). We have extracted from the web a list of bibliographical references. There are currently 15 800 entries in this specific KDB. Most of them come from the Pennsylvania University library and may be found at the following url: http://onlinebooks.library.upenn.edu/titles.html. We have also exploited shorter lists as the ones available at the url: http:The formulation of a question is not always as precise as who wrote the book " y " ?. Elliptic sentences as who wrote " y " ? or who is the author of " y " ? are more ambiguous. For instance, in Q8/196, " Hamlet " may be a movie or the famous play. The case is also encountered in Q11/1759: " Fiddler on the Roof " may be a novel or a musical. The novel was not in our KDB and it is a chance since only the musical has been considered as the correct answer by the judges. Whether we decide to enrich our resource or not, we have to take this kind of difficulty into account. 
Archives
It was also natural to check whether questions found in TREC-11 were not already present in previous TRECs. In such case, the answer provided could be reused. Let us call an Archive A i , a pair of two sets: (
Typos and Variants
Typos may be seen as a noise disturbing the canal between the input (Q) and the output (A). For a question such as Q10/1249/ Who wrote "The Devine Comedy"? the relation (Dante – Divine Comedy) included in the KDB described in 5.2.2 could not be exploited. We have used the classical edit distance 
KDB Summary
In the subsections 5.2.1 to 5.2.3, we have given some examples of the domains covered by the KDB we used. They correspond to about half of the answers currently supported by our KDB component. The second half concerns various topics such as rivers, mountains, Nobel's, hurricanes and so on. It is impossible to describe each of them in detail here, but it is interesting to see that the coverage is more or less the same on each TREC. 
Ordering answers
This year's QA track introduced newness in the evaluation measure in such a way that systems have to cope with the following principle: rank the answers from the most reliable to the less one. In order to take into account this requirement, our answers have been ordered according to results provided by the use (or non-use) of knowledge databases (KDB) – as a way to validate an answer – and by the question classifier output. So, for each question, the question classifier assigns one (or several) expected NE(s) and its (their) corresponding confidence(s). If it cannot be decided which of the 44 available entities should be responsive, the question is tagged as "unknown". From these points, our ordering strategy can be summarized schematically as follows: divide the Q-set in three main groups, @BULLET Q1: questions for which answers have been found by SIAC and validated with KDB. Since there is an agreement between two independent components, it is justified to assign a highest reliability score to the group produced by such a combination and to place it at the top ranks. Thirty-five questions were in this group and were ranked from 1 to 35. @BULLET Q2: questions for which answers have been found only by SIAC and not covered by any database. This group, the major one with 438 questions, could be divided in two parts: non NIL answers (389) and NIL answers (49). As described in section 4.3, filters are applied on SIAC output in order to keep only expected entities mapping question class(es) – it may happen that all the candidates are eliminated by this filtering – that is how NIL is produced by the system. It was decided to put these NIL at the end of this group, as they are the results of many treatments and therefore the decision process becomes too uncertain. Inside non NIL answers, order was defined first by decreasing confidences (in question classes) and second by question classes. Order among question classes (see 
This ordered list (Q1, Q2, Q3) corresponds to the way we ranked the three groups. YEAR, DATE, COUNTRY, COUNTY, NPP, ACRONYM, CITY, MAIL, MONTH, URL, STATE, ADDRESS, TITLE, LOCATION, ORGAN 
Experiments and results
Official results
Experiments
After the dead line, we performed some additional experiments. It was possible to evaluate them thanks to the TREC-11 answers patterns made available by Ken Litkowski. For this purpose, a home-made tool was developed to compute the confidence weighted score ( " CWS " ). In the following, these new experiments will be referred as LIA2002o (o standing for October) and LIA2002n (n for November). 
@BULLET Evaluation of the KDB contribution: 
The results reported in table 7.2 are useful to focus only on the behavior of questions for which a KDB was involved. For this, we assume that the other answers (from rank+1 to 500) were wrong: Where: " " R " , " U " , " CWS " stand respectively for " right " , " unsupported " , and " confidence weighted score " . " " Lia2002a " is the run submitted in August for TREC-11, " " Lia2002o " is a run with few additions in KDB. Also, minor bug corrections inside our whole system and specially in ordering strategy were done (ordering for SIAC answers was broken in our TREC submission) " " Lia2002n " is our last run. It includes two more entries (a tiny extension of the KDB). The main difference with the previous ones is that answers powered by KDB are ranked by applying the same ordering strategy as answers from SIAC. 
Conclusion
For our first participation in TREC -question answering, we focused on a small number of questions, that is questions for which an answer can be produced with a sufficient level of confidence. The goal was to reach 30% of accuracy which is honorable as a first trial. A lot of work remains. Firstly, we could have gone into entity recognition in greater depth, using more statistics. Secondly, because two different tools have been used in order to tag (NE) the documents and the questions, we experienced some problems making a mapping from one to the other. The lack of compatibility should be solved by using the same set of tag. Also, anaphora resolution is too simple and could be applied on many other anaphora phenomena. Another important point: question tagging is quite weak. For example, for many questions, it assigns the same confidence to different tags. The selection of the tag to be considered could be easily improved. Moreover, answer extraction is too much simple. Because no syntactic tagging is done, it is impossible to choose precisely a phrase in which the answer is supposed to be. So, the only thing we did was to extract the searched entity wherever it was in the candidate sentence. Consequently, many wrong answers were retrieved. Relying on some knowledge bases clearly improves the results of our system. Typo correction is quite efficient and allows us to answer correctly several questions. We can improve the cases where an answer is provided by the KDB and SIAC fails to retrieve any expected NE, by enriching the question with this answer in order to retrieve supporting documents. Moreover, we could increase the coverage of this KDB in two directions: i.) Find other knowledge sources on more and more subjects, ii.) Use each KDB as a bootstrap in order to enlarge it thru text extraction. We consider that the second item is a key point to make the first one feasible. In fact, there are 5 big stages: If we do not consider the first stage (due to the KDB), we have 4 stages (which length is between 11 and 33) where accuracy is quite good (between 0.25 and 0.36). This concentration is sufficiently significant to conclude that some questions have the same behavior and the system performs quite well on these types of questions. Since they are grouped, it should be possible to detect and locate them higher in the list. It could be possible to improve the results by detecting these types of questions. This concerns 48 questions, that is more than 90% of our correct answers. If they were located at the beginning of the list, the CWS would be 0.32 instead of 0.246. Sixth Message Understanding Conference, Columbia, Maryland: Morgan Kaufmann Publishers, pp. 55-69, 1995. 
Appendix
Hierarchical List of Expected Answer Types
