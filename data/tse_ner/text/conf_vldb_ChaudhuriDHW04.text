Introduction
Database systems support a simple Boolean query retrieval model, where a selection query on a SQL database returns all tuples that satisfy the conditions in the query. This often leads to the 
Consider a potential home buyer searching for homes in this database. A query with a not very selective condition such as " City=Seattle and View=Waterfront " may result in too many tuples in the answer, since there are many homes with waterfront views in Seattle. The Many-Answers Problem has been investigated outside the database area, especially in Information Retrieval (IR), where many documents often satisfy a given keyword-based query. Approaches to overcome this problem range from query reformulation techniques (e.g., the user is prompted to refine the query to make it more selective), to automatic ranking of the query results by their degree of " relevance " to the query (though the user may not have explicitly specified how) and returning only the top-K subset. It is evident that automated ranking can have compelling applications in the database context. For instance, in the earlier example of a homebuyer searching for homes in Seattle with waterfront views, it may be preferable to first return homes that have other desirable attributes, such as good school districts, boat docks, etc. In general, customers browsing product catalogs will find such functionality attractive. In this paper we propose an automated ranking approach for the Many-Answers Problem for database queries. Our solution is principled, comprehensive, and efficient. We summarize our contributions below. Any ranking function for the Many-Answers Problem has to look beyond the attributes specified in the query, because all answer tuples satisfy the specified conditions 1 . However, investigating unspecified attributes is particularly tricky since we need to determine what the user's preferences for these unspecified attributes are. In this paper we propose that the ranking function of a tuple depends on two factors: (a) a global score which captures the global importance of unspecified attribute values, and 1 In the case of document retrieval, ranking functions are often based on the frequency of occurrence of query values in documents (term frequency, or TF). However, in the database context, especially in the case of categorical data, TF is irrelevant as tuples either contain or do not contain a query value. Hence ranking functions need to also consider values of unspecified attributes. (b) a conditional score which captures the strengths of dependencies (or correlations) between specified and unspecified attribute values. For example, for the query " City = Seattle and View = Waterfront " , a home that is also located in a " SchoolDistrict = Excellent " gets high rank because good school districts are globally desirable. A home with also " BoatDock = Yes " gets high rank because people desiring a waterfront are likely to want a boat dock. While these scores may be estimated by the help of domain expertise or through user feedback, we propose an automatic estimation of these scores via workload as well as data analysis. For example, past workload may reveal that a large fraction of users seeking homes with a waterfront view have also requested for boat docks. The next challenge is how do we translate these basic intuitions into principled and quantitatively describable ranking functions? To achieve this, we develop ranking functions that are based on Probabilistic Information Retrieval (PIR) ranking models. We chose PIR models because we could extend them to model data dependencies and correlations (the critical ingredients of our approach) in a more principled manner than if we had worked with alternate IR ranking models such as the Vector-Space model. We note that correlations are often ignored in IR because they are very difficult to capture in the very high-dimensional and sparsely populated feature spaces of text data, whereas there are often strong correlations between attribute values in relational data (with functional dependencies being extreme cases), which is a much lower-dimensional, more explicitly structured and densely populated space that our ranking functions can effectively work on. The architecture of our ranking has a pre-processing component that collects database as well as workload statistics to determine the appropriate ranking function. The extracted ranking function is materialized in an intermediate knowledge representation layer, to be used later by a query processing component for ranking the results of queries. The ranking functions are encoded in the intermediate layer via intuitive, easy-to-understand " atomic " numerical quantities that describe (a) the global importance of a data value in the ranking process, and (b) the strengths of correlations between pairs of values (e.g., " if a user requests tuples containing value y of attribute Y, how likely is she to be also interested in value x of attribute X? " ). Although our ranking approach derives these quantities automatically, our architecture allows users and/or domain experts to tune these quantities further, thereby customizing the ranking functions for different applications. We report on a comprehensive set of experimental results. We first demonstrate through user studies on real datasets that our rankings are superior in quality to previous efforts on this problem. We also demonstrate the efficiency of our ranking system. Our implementation is especially tricky because our ranking functions are relatively complex, involving dependencies/correlations between data values. We use novel pre-computation techniques which reduce this complex problem to a problem efficiently solvable using Top-K algorithms. The rest of this paper is organized as follows. In Section 2 we discuss related work. In Section 3 we define the problem and outline the architecture of our solution. In Section 4 we discuss our approach to ranking based on probabilistic models from information retrieval. In Section 5 we describe an efficient implementation of our ranking system. In Section 6 we discuss the results of our experiments, and we conclude in Section 7. 
Permission 
Related Work
Extracting ranking functions has been extensively investigated in areas outside database research such as Information Retrieval. The vector space model as well as probabilistic information retrieval (PIR) models 
Problem Definition and Architecture
In this section, we formally define the Many-Answers Problem in ranking database query results, and also outline a general architecture of our solution. 
Problem Definition
We start by defining the simplest problem instance. Consider a database table D with n tuples {t 1 , …, t n } over a set of m categorical attributes A = {A 1 , …, A m }. Consider a " SELECT * FROM D " query Q with a conjunctive selection condition of the form " WHERE X 1 =x 1 AND … AND X s =x s " , where each X i is an attribute from A and x i is a value in its domain. The set of attributes X ={X 1 , …, X s }⊆ A is known as the set of attributes specified by the query, while the set Y = A – X is known as the set of unspecified attributes. Let S ⊆ {t 1 , …, t n } be the answer set of Q. The Many-Answers Problem occurs when the query is not too selective, resulting in a large S. The above scenario only represents the simplest problem instance. For example, the type of queries described above are fairly restrictive; we refer to them as point queries because they specify single-valued equality conditions on each of the specified attributes. In a more general setting, queries may contain range/IN conditions, and/or Boolean operators other than conjunctions. Likewise, databases may be multi-tabled, may contain a mix of categorical and numeric data, as well as missing or NULL values. While our techniques extend to all these generalizations, in the interest of clarity (and due to lack of space), the main focus of this paper is on ranking the results of conjunctive point queries on a single categorical table (without NULL values). 
General Architecture of our Approach
In the next section we discuss PIR-based ranking functions for structured data. 
Ranking Functions: Adaptation of PIR Models for Structured Data
In this section we discuss PIR-based ranking functions, and then show how they can be adapted for structured data. We discuss the semantics of the atomic building blocks that are used to encode these ranking functions in the intermediate layer. We also show how these atomic numerical quantities can be estimated from a variety of knowledge sources, such as data and workload statistics, as well as domain knowledge. 
Review of Probabilistic Information Retrieval
Much of the material of this subsection can be found in textbooks on Information Retrieval, such as 
Bayes' Rule: 
Product Rule: 
Consider a document collection D. For a (fixed) query Q, let R represent the set of relevant documents, and R =D – R be the set of irrelevant documents. In order to rank any document t in D, we need to find the probability of the relevance of t for the query given the text features of t (e.g., the word/term frequencies in t), i.e., p(R|t). More formally, in probabilistic information retrieval, documents 
) ( 
) ( ) | ( ) | ( b p a p a b p b a p = ) , | ( ) | ( ) | , ( c a b p c a p c b a p = 
Query Processing 
Workload are ranked by decreasing order of their odds of relevance, defined as the following score: 
The main issue is, how are these probabilities computed, given that R and R are unknown at query time? The usual techniques in IR are to make some simplifying assumptions, such as estimating R through user feedback, approximating R as D (since R is usually small compared to D), and assuming some form of independence between query terms (e.g., the Binary Independence Model). In the next subsection we show how we adapt PIR models for structured databases, in particular for conjunctive queries over a single categorical table. Our approach is more powerful than the Binary Independence Model as we also leverage data dependencies. 
Adaptation of PIR Models for Structured Data
In our adaptation of PIR models for structured databases, each tuple in a single database table D is effectively treated as a " document " . For a (fixed) query Q, our objective is to derive Score(t) for any tuple t, and use this score to rank the tuples. Since we focus on the Many- Answers problem, we only need to concern ourselves with tuples that satisfy the query conditions. Recall the notation from Section 3.1, where X is the set of attributes specified in the query, and Y is the remaining set of unspecified attributes. We denote any tuple t as partitioned into two parts, t(X) and t(Y), where t(X) is the subset of values corresponding to the attributes in X, and t(Y) is the remaining subset of values corresponding to the attributes in Y. Often, when the tuple t is clear from the context, we overload notation and simply write t as consisting of two parts, X and Y (in this context, X and Y are thus sets of values rather than sets of attributes). Replacing t with X and Y (and R as D as mentioned in Section 4.1 is commonly done in IR), we get Since for the Many-Answers problem we are only interested in ranking tuples that satisfy the query conditions, and all such tuples have the same X values, we can treat any quantity not involving Y as a constant. We thus get 
) , | ( ) , | ( ) ( D X Y p R X Y p t Score ∝ 
Furthermore, the relevant set R for the Many-Answers problem is a subset of all tuples that satisfy the query conditions. One way to understand this is to imagine that R is the " ideal " set of tuples the user had in mind, but who only managed to partially specify it when preparing the query. Consequently the numerator 
) , | ( R X Y p may be replaced by ) | ( R Y p 
. We thus get We are not quite finished with our derivation of Score(t) yet, but let us illustrate Equation 1 with an example. Consider a query with condition " City=Kirkland and Price=High " (Kirkland is an upper class suburb of Seattle close to a lake). Such buyers may also ideally desire homes with waterfront or greenbelt views, but homes with views looking out into streets may be somewhat less desirable. Thus, p(View=Greenbelt | R) and p(View=Waterfront | R) may both be high, but p(View=Street | R) may be relatively low. Furthermore, if in general there is an abundance of selected homes with greenbelt views as compared to waterfront views, (i.e., the denominator p(View=Greenbelt | City=Kirkland, Price=High, D) is larger than p(View=Waterfront | City=Kirkland, Price=High, D)), our final rankings would be homes with waterfront views, followed by homes with greenbelt views, followed by homes with street views. Note that for simplicity, we have ignored the remaining unspecified attributes in this example. 
( | ) ( , | ) ( ) ( | ) ( , | ) ( | ) ( | , ) ( | ) ( | , ) p t R p X Y R Score t p t D p X Y D p X R p Y X R p X D p Y X D ∝ = ∝ ) | ( ) | ( ) ( ) ( ) | ( ) ( ) ( ) | ( ) | ( ) | ( ) ( R t p R t p t p R p R t p t p R p R t p t R p t R p t Score ∝ = = ) , | ( ) | ( ) ( D X Y p R Y p t Score ∝ (1
∏ ∈Y y C y p ) | ( 
) where C is any condition that only involves Y values (resp. X values), R, or D. While this assumption is patently false in many cases (for instance, in the example in Section 4.2 this assumes that there is no dependency between homes in Kirkland and high-priced homes), nevertheless the remaining dependencies that we do leverage, i.e., between the specified and unspecified values, prove to be significant for ranking. Moreover, as we shall show in Section 5, the resulting simplified functional form of the ranking function enables the efficient adaptation of known Top-K algorithms through novel data structuring techniques. We continue the derivation of the score of a tuple under the above assumptions: 
This simplifies to Although Equation 2 represents a simplification over Equation 1, it is still not directly computable, as R is unknown. We discuss how to estimate the quantities ) | ( R y p next. 
Workload-Based Estimation of p(y|R)
Estimating the quantities 
) | ( R y p requires knowledge of R, which is unknown at query time. The usual technique for estimating R in IR is through user feedback (relevance feedback) at query time, or through other forms of training. In our case, we provide an automated approach that leverages available workload information for estimating 
) | ( R y p 
. We assume that we have at our disposal a workload W, i.e., a collection of ranking queries that have been executed on our system in the past. We first provide some intuition of how we intend to use the workload in ranking. Consider the example in Section 4.2 where a user has requested for high-priced homes in Kirkland. The workload may perhaps reveal that, in the past a large fraction of users that had requested for high-priced homes in Kirkland had also requested for waterfront views. Thus for such users, it is desirable to rank homes with waterfront views over homes without such views. We note that this dependency information may not be derivable from the data alone, as a majority of such homes may not have waterfront views (i.e., data dependencies do not indicate user preferences as workload dependencies do). Of course, the other option is for a domain expert (or even the user) to provide this information (and in fact, as we shall discuss later, our ranking architecture is generic enough to allow further customization by human experts). More generally, the workload W is represented as a set of " tuples " , where each tuple represents a query and is a vector containing the corresponding values of the specified attributes. Consider an incoming query Q which specifies a set X of attribute values. We approximate R as all query " tuples " in W that also request for X. This approximation is novel to this paper, i.e., that all properties of the set of relevant tuples R can be obtained by only examining the subset of the workload that contains queries that also request for X. So for a query such as " City=Kirkland and Price=High " , we look at the workload in determining what such users have also requested for often in the past. We can thus write, for query Q, with specified attribute set X, This can be finally rewritten as: 
) | ( R y p as ) , | ( W X y p . Making this substitution in Equation 2, we get ∏∏ ∏ ∏ ∏∏ ∏ ∈ ∈ ∈ ∈ ∈ ∈ ∈ ∝ = Y y X x Y y X x Y y X x Y y D y x p D y p W y x p W y p D y x p D y p y p y W X p y W p ) , | ( 1 ) | ( ) , | ( ) | ( ) , | ( 1 ) | ( ) ( ) , | ( ) | ( ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∈ ∈ ∈ ∈ ∈ ∈ ∈ ∈ ∈ ∈ ∝ = = ∝ = ∝ Y y X x Y y Y y X x Y y Y y Y y Y y Y y D y x p D p y p y D p R y p D y x p y p y D p R y p y p D y X p y D p R y p y p y D X p R y p D X p y p y D X p R y p D X y p R y p t Score ) , | ( 1 ) ( ) ( ) | ( ) | ( ) , | ( 1 ) ( ) | ( ) | ( ) ( ) , | ( ) | ( ) | ( ) ( ) | , ( ) | ( ) , ( ) ( ) | , ( ) | ( ) , | ( ) | ( ) ( ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∏ ∈ ∈ ∈ ∈ ∈ ∈ ∈ ∈ ∈ ∝ = ∝ Y y X x Y y Y y X x Y y Y y X x Y y D y x p D y p y p y W X p D y x p D y p W X p y p y W X p D y x p D y p W X y p t Score ) , | ( 1 ) | ( ) ( ) | , ( ) , | ( 1 ) | ( ) , ( ) ( ) | , ( ) , | ( 1 ) | ( ) , | ( ) ( ∏∏ ∏ ∈ ∈ ∈ ∝ Y y X x Y y D y x p D y p R y p t Score ) , | ( 1 ) | ( ) | ( ) ( 
Equation 3 is the final ranking formula that we use in the rest of this paper. Note that unlike Equation 2, we have effectively eliminated R from the formula, and are only left with having to compute quantities such as 
) | ( W y p , ) , | ( W y x p , ) | ( D y p , and ) , | ( D y x p 
. In fact, these are the " atomic " numerical quantities referred to at various places earlier in the paper. Also note that the score in Equation 3 is composed of two large factors. The first factor may be considered as the global part of the score, while the second factor may be considered as the conditional part of the score. Thus, in the example in Section 4.2, the first part measures the global importance of unspecified values such as waterfront, greenbelt and street views, while the second part measures the dependencies between these values and specified values " City=Kirkland " and " Price=High " . 
Computing the Atomic Probabilities
Our strategy is to pre-compute each of the atomic quantities for all distinct values in the database. The quantities 
) | ( W y p and ) | ( D y p 
are simply the relative frequencies of each distinct value y in the workload and database, respectively (the latter is similar to IDF, or the inverse document frequency concept in IR), while the quantities 
) , | ( W y x p and ) , | ( D y x p 
may be estimated by computing the confidences of pair-wise association rules 
Implementation
In this section we discuss the implementation of our database ranking system. 
Pre-Processing
This component is divided into several modules. First, the Atomic Probabilities Module computes the quantities 
) | ( W y p , ) | ( D y p , ) , | ( W y x p , and ) , | ( D y x p 
for all distinct values x and y. These quantities are computed by scanning the workload and data, respectively (while the latter two quantities can be computed by running a general association rule mining algorithm such as 
∏∏ ∏ ∈ ∈ ∈ ∝ Y y X x Y y D y x p W y x p D y p W y p t Score ) , | ( ) , | ( ) | ( ) | ( ) ( (3) 
with appropriate indexes to enable easy retrieval. In particular, 
) | ( W y p and ) | ( D y p 
are respectively stored in two tables, each with columns {AttName, AttVal, Prob} and with a composite B+ tree index on (AttName, AttVal), while 
) , | ( W y x p and ) , | ( D y x p 
respectively are stored in two tables, each with columns {AttNameLeft, AttValLeft, AttNameRight, AttValRight, Prob} and with a composite B+ tree index on (AttNameLeft, AttValLeft, AttNameRight, AttValRight). These atomic quantities can be further customized by human experts if necessary. This intermediate layer now contains enough information for computing the ranking function, and a naïve query processing algorithm (henceforth referred to as the Scan algorithm) can indeed be designed, which, for any query, first selects the tuples that satisfy the query condition, then scans and computes the score for each such tuple using the information in this intermediate layer, and finally returns the Top-K tuples. However, such an approach can be inefficient for the Many-Answers problem, since the number of tuples satisfying the query condition can be very large. At the other extreme, we could pre-compute the Top-K tuples for all possible queries (i.e., for all possible sets of values X), and at query time, simply return the appropriate result set. Of course, due to the combinatorial explosion, this is infeasible in practice. We thus pose the question: how can we appropriately trade off between pre-processing and query processing, i.e., what additional yet reasonable precomputations are possible that can enable faster queryprocessing algorithms than Scan? The high-level intuition behind our approach to the above problem is as follows. Instead of pre-computing the Top-K tuples for all possible queries, we pre-compute ranked lists of the tuples for all possible " atomic " queries -each distinct value x in the table defines an atomic query Qx that specifies the single value {x}. Then at query time, given an actual query that specifies a set of values X, we " merge " the ranked lists corresponding to each x in X to compute the final Top-K tuples. Of course, for this high-level idea to work, the main challenge is to be able to perform the merging without having to scan any of the ranked lists in its entirety. One idea would be to try and adapt well-known Top-K algorithms such as the Threshold Algorithm (TA) and its derivatives 
∏ ∈ = t z D z x p W z x p CondScore ) , | ( ) , | ( 
where z ranges over all attribute values of t. 2. Global List G x : This list consists of pairs of the form <TID, GlobScore>, ordered by descending GlobScore, where TID is the tuple-id of a tuple t that contains x and 
∏ ∈ = t z D z p W z p GlobScore ) | ( ) | ( 
These lists enable efficient computation of the score of a tuple t for any query as follows: given query Q specifying conditions for a set of attribute values, say X = {x 1 ,..,x s }, at query time we retrieve and multiply the scores of t in the lists C x1 ,…,C xs and in one of G x1 ,…,G xs . This requires only s+1multiplications and results in a score 2 that is proportional to the actual score. Clearly this is more efficient than computing the score " from scratch " by retrieving the relevant atomic probabilities from the intermediate layer and composing them appropriately. We need to enable two kinds of access operations efficiently on these lists. First, given a value x, it should be possible to perform a GetNextTID operation on lists C x and G x in constant time, i.e., the tuple-ids in the lists should be efficiently retrievable one-by-one in order of decreasing score. This corresponds to the sorted stream access of TA. Second, it should be possible to perform random access on the lists, i.e., given a TID, the corresponding score (CondScore or GlobScore) should be retrievable in constant time. To enable these operations efficiently, we materialize these lists as database tables – all the conditional lists are maintained in one table called CondList (with columns {AttName, AttVal, TID, CondScore}) while all the global lists are maintained in another table called GlobList (with columns {AttName, AttVal, TID, GlobScore}). The table have composite B+ tree indices on (AttName, AttVal, CondScore) and (AttName, AttVal, GlobScore) respectively. This enables efficient performance of both access operations. Further details of how these data structures and their access methods are used in query processing are discussed in Section 5.2. 
Query Procesing Component
In this subsection we describe the query processing component. The naïve Scan algorithm has already been described in Section 5.1, so our focus here is on the alternate List Merge algorithm (see 
Section 5.1, is proportional to the actual score derived in Equation 3). The termination criterion guarantees that no more GetNextTID operations will be needed on any of the lists. This is accomplished by maintaining an array T which contains the last scores read from all the lists at any point in time by GetNextTID operations. The product of the scores in T represents the score of the very best tuple we can hope to find in the data that is yet to be seen. If this value is no more than the tuple in the Top-K buffer with the smallest score, the algorithm successfully terminates. 
Limited Available Space
So far we have assumed that there is enough space available to build the conditional and global lists. A simple analysis indicates that the space consumed by these lists is O(mn) bytes (m is the number of attributes and n the number of tuples of the database table). However, there may be applications where space is an expensive resource (e.g., when lists should preferably be held in memory and compete for that space or even for space in the processor cache hierarchy). We show that in such cases, we can store only a subset of the lists at preprocessing time, at the expense of an increase in the query processing time. Determining which lists to retain/omit at preprocessing time may be accomplished by analyzing the workload. A simple solution is to store the conditional lists C x and the corresponding global lists G x only for those attribute values x that occur most frequently in the workload. At query time, since the lists of some of the specified attributes may be missing, the intuitive idea is to probe the intermediate knowledge representation layer (where the " relatively raw " data is maintained, i.e., the 
List Merge Algorithm 
Input: Query, data table, global and conditional lists Output: Top-K tuples Let Gxb be the shortest list among Gx1,…,Gxs Let B ={} be a buffer that can hold K tuples ordered by score Let T be an array of size s+1 storing the last score from each list Initialize B to empty REPEAT More specifically, we use a modification of TA described in 
UNTIL ∏ + = ≥ 1 1 ] [ ]. [ s i i T Score K B RETURN B 
Index Module 
∏ ∈ = t z D z x p W z x p CondScore ) , | ( ) , | ( Add <TID, CondScore> to C x ∏ ∈ = t z D z p W z p GlobScore ) | ( ) | 
Experiments
In this section we report on the results of an experimental evaluation of our ranking method as well as some of the competitors. We evaluated both the quality of the rankings obtained, as well as the performance of the various approaches. We mention at the outset that preparing an experimental setup for testing ranking quality was extremely challenging, as unlike IR, there are no standard benchmarks available, and we had to conduct user studies to evaluate the rankings produced by the various algorithms. For our evaluation, we use real datasets from two different domains. The first domain was the MSN HomeAdvisor database (http://houseandhome.msn.com/), from which we prepared a table of homes for sale in the US, with attributes such as Price, Year, City, Bedrooms, Bathrooms, Sqft, Garage, etc. (we converted numerical attributes into categorical ones by discretizing them into meaningful ranges). The original database table also had a text column called Remarks, which contained descriptive information about the home. From this column, we extracted additional Boolean attributes such as Fireplace, View, Pool, etc. To evaluate the role of the size of the database, we also performed experiments on a subset of the HomeAdvisor database, consisting only of homes sold in the Seattle area. The second domain was the Internet Movie Database (http://www.imdb.com), from which we prepared a table of movies, with attributes such as Title, Year, Genre, Director, FirstActor, SecondActor, Certificate, Sound, Color, etc. (we discretized numerical attributes such as Year into meaningful ranges). We first selected a set of movies by the 30 most prolific actors for our experiments. From this we removed the 250 most well-known movies, as we did not wish our users to be biased with information they already might know about these movies, especially information that is not captured by the attributes that we had selected for our experiments. The sizes of the various (single-table) datasets used in our experiments are shown in 
Quality Experiments
We evaluated the quality of two different ranking methods: (a) our ranking method, henceforth referred to as Conditional (b) the ranking method described in 
Examples of Ranking Results
For the Seattle Homes dataset, both Conditional as well as Global produced rankings that were intuitive and reasonable. There were interesting examples where Conditional produced rankings that were superior to Global. For example, for a query with condition " City=Seattle and Bedroom=1 " , Conditional ranked condos with garages the highest. Intuitively, this is because private parking in downtown is usually very scarce, and condos with garages are highly sought after. However, Global was unable to recognize the importance of garages for this class of homebuyers, because most users (i.e., over the entire workload) do not explicitly request for garages since most homes have garages. As another example, for a query such as " Bedrooms=4 and City=Kirkland and Price=Expensive " , Conditional ranked homes with waterfront views the highest, whereas Global ranked homes in good school districts the highest. This is as expected, because for very rich homebuyers a waterfront view is perhaps a more desirable feature than a good school district, even though the latter may be globally more popular across all homebuyers. Likewise, for the Movies dataset, Conditional often produced rankings that were superior to Global. For example, for a query such as " Year=1980s and Genre=Thriller " , Conditional ranked movies such as " Indiana Jones and the Temple of Doom " higher than " Commando " , because the workload indicated that Harrison Ford was a better known actor than Arnold Schwarzenegger during that era, although the latter actor was globally more popular over the entire workload. 
Ranking Evaluation
We now present a more formal evaluation of the ranking quality produced by the ranking algorithms. We conducted two surveys; the first compared the rankings against user rankings using standard precision/recall metrics, while the second was a simpler survey that asked users to rate which algorithm's rankings they preferred. 
Average Precision
: Since requiring users to rank the entire database for each query would have been extremely tedious, we used the following strategy. For each dataset, we generate 5 test queries. For each test query Q i we generated a set H i of 30 tuples likely to contain a good mix of relevant and irrelevant tuples to the query. We did this by mixing the Top-10 results of both the Conditional and Global ranking algorithms, removing ties, and adding a few randomly selected tuples. Finally, we presented the queries along with their corresponding H i 's (with tuples randomly permuted) to each user in our study. Each user's responsibility was to mark 10 tuples in H i as most relevant to the query Q i . We then measured how closely the 10 tuples marked as relevant by the user (i.e., the " ground truth " ) matched the 10 tuples returned by each algorithm. We used the formal Precision/Recall metrics to measure this overlap. Precision is the ratio of the number of retrieved tuples that are relevant, to the total number of retrieved tuples, while Recall is the fraction of the number of retrieved tuples that are relevant, to the total number of relevant tuples (see 
Seattle Homes 
User Preference of Rankings: 
In this experiment, for the Seattle Homes as well as the Movies dataset, users were given the Top-5 results of the two ranking methods for 5 queries (different from the previous survey), and were asked to choose which rankings they preferred. 
Performance Experiments
In this subsection we report on experiments that compared the performance of the various implementations of the Conditional algorithm: List Merge, its space-saving variants, and Scan. We do not report on the corresponding implementations of Global as they had similar performance. We used the Seattle Homes and US Homes datasets for these experiments. 
Preprocessing Time and Space: 
Since the preprocessing performance of the List Merge algorithm is dominated by the Index Module, we omit reporting results for the Atomic Probabilities Module. 
Datasets 
Space Saving Variations: 
In this experiment we show how the performance of the algorithms changes when only a subset of the set of global and conditional lists are stored. Recall from Section 5.2.1 that we only retain lists for the values of the frequently occurring attributes in the workload. For this experiment we consider Top-10 queries with selection conditions that specify two attributes (queries generated by randomly picking a pair of attributes and a domain value for each attribute), and measure their execution times. The compared algorithms are: @BULLET LM: List Merge with all lists available @BULLET LMM: List Merge where lists for one of the two specified attributes are missing, halving space @BULLET Scan 
NumSelected 
Conclusions
We proposed a completely automated approach for the Many-Answers Problem which leverages data and workload statistics and correlations. Our ranking functions are based upon the probabilistic IR models, judiciously adapted for structured data. We presented results of preliminary experiments which demonstrate the efficiency as well as the quality of our ranking system. Our work brings forth several intriguing open problems. For example, many relational databases contain text columns in addition to numeric and categorical columns. It would be interesting to see whether correlations between text and non-text data can be leveraged in a meaningful way for ranking. Finally, comprehensive quality benchmarks for database ranking need to be established. This would provide future researchers with a more unified and systematic basis for evaluating their retrieval algorithms. 
