INTRODUCTION
As sensor, communication, and computing technologies continue to evolve, it is becoming increasingly feasible and attractive to monitor, record, and query data that capture the evolving states of continuously changing real-world phenomena. As an example that is representative of this new class of updateintensive applications, consider advanced location-based services, e.g., in the context of intelligent transport systems, where the positions of a large population of GPS-equipped moving objects are tracked on a server. To maintain accurately the current positions of such objects, the positions must be updated frequently. Experiments with GPS data from vehicles traveling in semi-urban environments show that to know the positions of the vehicles being tracked with an accuracy of 200 meters requires an update from each vehicle on average every 15 seconds 
PROBLEM SETTING
Data And Update Operations
 Consider a location-based service or fleet management application where a server tracks the current positions of a population of moving objects. It is a fundamental observation that, due to the discrete sampling of the movements and the inherent inaccuracy of positioning technologies, the positions of the objects cannot be known accurately at the server. Instead, the position of an object o may be assumed to be known with an accuracy thr o 
The R-Tree
The R-tree is a height-balanced tree 
a b K c d L X g e M j h N Y f K L a b c d e f g h j M N X Y 
DATA STRUCTURE AND ALGORITHMS
The R R -Tree Data Structure
The R R -tree data structure consists of two R-trees: a disk-based tree and an operation-buffer tree in main memory. The disk-based tree is a standard R-tree with the identification tuples introduced in Section 2 as leaf-node entries. In contrast, the operation-buffer tree stores operations yet to be performed on the disk-based tree. Its leaf-node entries thus consist of identification tuples augmented with deletion flags that indicate whether an entry represents an insertion or a deletion. In the following, if not specified otherwise, the terms entry and node refer to entries and nodes of the disk-based R-tree. The operation buffer has room for Cmax pending insertion and deletion operations, and the current number of operations in the buffer is denoted by C, C ≤ Cmax. Since flushing of the operation buffer to the disk-based tree is done only at certain points in time, the buffer might contain operations that invalidate some of the data in the disk tree. The property below characterizes the relation between the data on disk and the operations in the buffer. We use ω d to denote a data entry in the disk-based R-tree, and ω + m (ω − m ) to denote a corresponding insertion (deletion) operation in the mainmemory operation buffer. Property 1. For any given identification tuple ω = objId , p from the sequence of operations, exactly one of the following four cases is true at any given time. If the last operation concerning ω was a deletion, (1) there is no ω d , no ω + m , and no ω − m , or (2) there is a ω d and an ω − m . If the last operation concerning ω was an insertion, (3) there is no ω d , but there is an ω + m , or (4) there is an ω d , and there is no ω + m and no ω − m . 
Note that if no operations concerning ω have yet been performed, case 1 of the property holds trivially. 
..., 
... 2: EmptyBuffer Piggyback ..., 〈id, p〉 + , ... 
... 
3: 
After 〈id, p〉 + EmptyBuffer Piggyback 〈id, p〉 + ...,〈id, p〉, ... 
... 
Insertion and Deletion Operations
Insertions and deletions are implemented by the same UPDATE algorithm, using the delFlag parameter to distinguish the two types of operations (see Algorithm 1). This algorithm first checks whether the operation buffer (buf ) has room for the current update operation and empties part of the buffer if necessary (EMPTYBUFFER). Then it checks if the opposite operation already exists in the buffer by querying it. An opposite operation is defined as the operation with the same p and objId , but with the opposite deletion flag value. If such an operation is found, the previous operation is removed from the buffer and the incoming operation is ignored. 
We call this an 
Algorithm 1 The UPDATE algorithm UPDATE(objId , p, delFlag) 1 if buf is full 2 then EMPTYBUFFER 3 oppositeEntry ← objId , p, ¬delFlag 4 if buf .exists(oppositeEntry) 5 then buf .remove(oppositeEntry) 6 else buf .insert(objId , p, delFlag) annihilation. Otherwise, if the opposite operation is not found, the incoming operation is inserted into the operation buffer. The lemma that follows implies that the annihilation algorithm is correct with respect to Property 1. LEMMA 1. Property 1 holds after invocation of the UPDATE algorithm if C < Cmax holds as a precondition (the buffer is not full and EMPTYBUFFER is not called). PROOF. Proof by induction. In the case of a newly created, empty tree, Property 1 holds trivially. Assume that Property 1 holds before an UPDATE. Next assume that we are about to perform an insertion for ω = objId , p. This means that the most recent update for ω was a deletion or this is the first insertion for ω. As illustrated in 
{ω d , ω − m } + ω + = ω d . 
 If the processed operation is a deletion then the most recent update was an insertion and the index is either in state 3 or in state 4. In state 3, processing ω − results in the most common case of annihilation, transforming the state of the structure into state 1: 
ω + m + ω − = ∅. 
Finally, in state 4, ω − is simply added to the buffer, transforming the index into state 2: 
ω d + ω − = {ω d , ω − m }. 
While the annihilation in state 3 is intuitive, to better understand the intuition of annihilation in state 2, consider what would happen if the involved operations were allowed to complete on the disk tree one by one. The deletion from the buffer first removes the data from the tree; then, later, the insertion creates an entry that is identical to the one removed. The combined effect is that the tree transforms into state 4, the effect of which is identical to that of the annihilation. 
EmptyBuffer Algorithm
 The EMPTYBUFFER algorithm, presented in Algorithm 2 1 , is responsible for committing some of the operations in the buffer todisk. Note that although all operations are removed from the buffer in line 2 of the pseudo code, some of them are usually put back into the buffer in the GROUPUPDATE algorithm, described in Section 3.4. In addition, with the appropriate bookkeeping, the implementation of the algorithm needs not materialize the opList as is done in the pseudo code. 
Algorithm 2 The EMPTYBUFFER algorithm EMPTYBUFFER() 1 opList ← buf.GetAllOperations 2 buf.remove(opList) 3 siblings ← GROUPUPDATE(rootN ode, opList) 4 while |siblings| > 1 5 do rootN ode ← newN odeW ithEntries(siblings) 6 siblings ← GROUPSPLIT(rootN ode) 
 The GROUPUPDATE algorithm performs the argument operations on the root node of the disk tree and returns a set of root-level sibling nodes, i.e., subtrees of the same height as the disk tree, including the original root. If this set contains other nodes than the original root, the tree is grown by creating a new root. It is possible that there were so many sibling entries that the new root overflows. In that case, the new root is split, which grows the tree further. The GROUPSPLIT algorithm used in line 6 (and as a subroutine of GROUPUPDATE) checks whether the node is overflowing. If not, it returns the original node; otherwise, the node is repeatedly split by the regular R*-tree split algorithm 
GroupUpdate Algorithm
The GROUPUPDATE, presented in Algorithm 3, is the core of the proposed indexing technique. It executes the specified operation list (a sublist of the operations formerly present in the buffer) on the specified subtree. 
Algorithm 3 The GROUPUPDATE algorithm GROUPUPDATE(node, opList) 1 if node is leaf 2 then EXECUTEENTRIES(node, opList) 3 else pushG ← GETPUSHDOWNOPS(opList, node) 4 regroup ← false 5 for each child, opSublist in pushG 6 do if regroup 7 then return GROUPUPDATE(node, pushG) 8 pushG ← pushG \ opSublist 9 node.remove(child) 10 newCh ← GROUPUPDATE(child, opSublist) 11 if newCh = {newChild} 12 then regroup 13 ← INTEGRATECHILD(node, newChild) 14 else node.add(newChildren) 15 if |node| = 1 and node is non-leaf 16 then node ← node
If the subtree is a leaf node then the operation list is executed on the node trivially by means of EXECUTEENTRIES, which just adds insertion entries to the node and removes existing entries matched by deletion entries. If the subtree is rooted at a non-leaf node, GET- PUSHDOWNOPS returns a mapping from its child nodes to the individual operation sublists that should be executed on these nodes. As will be discussed later, some of the operations from opList can also be put back into the buffer and are thus not processed during this buffer emptying. Each mapping in pushG is then executed in turn by a recursive GROUPUPDATE call (line 10). This results in a list of new child nodes in newCh. The rest of the algorithm is concerned with integrating these nodes into the tree. In the simplest case, there are two or more such nodes. Their entries are trivially added to the current node. If only one child node is returned, further and more complicated treatment may be necessary in case the child is underfull or has shrunk in height. This is handled by INTEGRATE- CHILD and is discussed in the next section. This algorithm may change other sibling child entries, invalidating the remaining, nonprocessed mappings in pushG. This is signaled by the regroup flag, which forces a restart of GROUPUPDATE. It is guaranteed that at least one mapping is completed before such a restart can occur; thus, the algorithm is guaranteed to terminate. After performing all the operations, the current node might contain only one entry, in which case it is replaced by its child in the tree, thus shrinking the tree in height. The node may also end up overfull. It is then passed to GROUPSPLIT, which returns a list of non-overflowing nodes. This list is returned as the result of the algorithm. The next lemma states that the UPDATE algorithm complies with Property 1 when EMPTYBUFFER is called. The proof assumes that those operations that reach the leaves of the disk tree are correctly routed there by the GETPUSHDOWNOPS algorithm. LEMMA 2. Property 1 holds after invocation of the UPDATE algorithm if C = Cmax holds as a precondition (the buffer is full). PROOF. According to the Lemma 1, Property 1 holds as a precondition . For every identification pair ω that is contained in the buffer, the tree might be in either state 2 or state 3. EMPTYBUFFER will either leave the ω in the buffer, or the operation will be performed correctly on the disk tree and removed from the buffer. In the latter case, the index will move from state 2 to state 1, or from state 3 to state 4 (see 
IntegrateChild Algorithm
The INTEGRATECHILD algorithm (Algorithm 4) is responsible for adding an arbitrarily-looking R-tree to the current node. Several cases must be handled. One case is where the root of the new tree is the only child of the current node. Then the current node is removed and the child node takes its place. More commonly, the root of the new tree has siblings in the current node and is either underfull or normal. The contents of an underfull node are merged into the sibling nodes by the MERGESUBTREES call, whereas a normal node is inserted into tree by INSERTSUBTREE, as discussed shortly. Subroutines INSERTSUBTREE and MERGESUBTREES serve similar purpose—putting a subtree into the main tree—and have similar implementations. The principal difference is that INSERTSUB- TREE handles a subtree whose root is a normal (not underfull) node, whereas MERGESUBTREES handles the case of an underfull node. 
Algorithm 4 The INTEGRATECHILD algorithm INTEGRATECHILD(node, newCh) 1 if |newCh| = 0 2 then if |node| = 0 3 then node ← newCh 4 else if newCh is underfull 5 then 6 return MERGESUBTREES(node, newCh) 7 else 8 return INSERTSUBTREE(node, newCh) 9 return mustNotRegroup The INSERTSUBTREE (Algorithm 5) checks whether the specified node is at the right level for insertion of the subtree, i.e., the distance to the leaf level from the node in the main tree must be equal to the subtree height. 
Algorithm 5 INSERTSUBTREE algorithm INSERTSUBTREE(node, subtree) 1 if subtree is one level below node 2 then node.add(subtree) 3 else childNode ← CHOOSESUBTREE(node, subtree) 4 INSERTSUBTREE(childNode, subtree) 5 childNodes ← GROUPSPLIT(childNode) 6 node.add(childNodes) 7 if |childNodes| > 1 8 then return mustRegroup 9 return mustNotRegroup If so, a new entry for the subtree is created and inserted into the node. This case is the most common, and it does not incur any additional I/O. However, if the specified node is not at the right level to accommodate the subtree, then, using the minimum bounding rectangle of the subtree, the standard R * -tree CHOOSESUBTREE algorithm is called to identify an appropriate node one level below, and INSERTSUBTREE is called recursively. After returning from the recursive call, GROUPSPLIT is called on the potentially overflowing child node, and the resulting entries are inserted into the parent node. Since GROUPSPLIT may replace one child node with two new child nodes, it invalidates any parent GROUPUPDATE mappings for this node. This situation is signaled to the caller by the return value. The MERGESUBTREES algorithm is very similar to INSERTSUB- TREE. The only difference is that when the node at the right level is found, it is merged with the underfull node passed as an argument to the algorithm. To complete the definition of the update algorithms, the GET- PUSHDOWNOPS algorithm is discussed next. It is used in algorithm GROUPUPDATE, to route groups of operations down the disk tree. 
GetPushDownOps Algorithm
 Given an operation list and a node of the disk tree, the GET- PUSHDOWNOPS algorithm (Algorithm 6) determines which of the operations should be pushed down and to which children of the node. First, the GROUPOPERATIONS subroutine is called to establish the mapping between the operations and the child nodes according to the usual R * -tree rules (see Algorithm 7). The mapping returned is a list of childNode, opSublist tuples . Each insertion is mapped to a single child node according the R * -tree's CHOOSESUBTREE algorithm 
Algorithm 6 The GETPUSHDOWNOPS algorithm GETPUSHDOWNOPS(node, opList) 1 mapping ← GROUPOPERATIONS(node, opList) 2 if node is the root 3 then for each chNode, sublist in mapping 4 do if |sublist| < k 5 then 6 mapping ← mapping \ {{chNode, sublist} 7 buf .insert(sublist) 8 return mapping Algorithm 7 The GROUPOPERATIONS algorithm GROUPOPERATIONS(node, opList) 1 opMapping ← ∅ 2 for each op in opList 3 do if op is a deletion 4 then matchingChildren ← Covering(node, op) 5 for each chNode in matchingChildren 6 do opMapping ← opMapping ∪ {op → chNode} 7 else childNode ← CHOOSESUBTREE(node, op) 8 opMapping ← opMapping ∪ {op → childNode} 9 return opM apping child nodes whose bounding rectangle includes it (as returned by the Covering method in line 4 of the algorithm). As shown in Algorithm 6, if the node is not the root, the algorithm just returns the mapping obtained from the call to GROUP- OPERATIONS. If the node is the root, the algorithm removes from the mapping all operation sublists that have fewer than k operations . Operations in these sublists are put back into the buffer. The idea using an operation threshold for the buffer emptying is to avoid spending expensive I/O on the small groups, so that as many operations as possible share the same I/O. Choosing a threshold k is not trivial. If a large value for k is chosen, this will cause only very few groups to be emptied, leading in turn to the need for very frequent buffer emptying. Choosing a small k value also has obvious drawbacks. The analytical cost modeling in Section 4 provides a way to choose an optimal value of k. Note that because some sublists of operations are put back into the buffer, one copy of a deletion may be put back into the buffer while another is sent down the tree. For this reason, if the deletion succeeds to delete an entry in the index tree, the EXECUTEEN- TRIES algorithm (called from GROUPUPDATE) removes the corresponding copy of the deletion operation from the buffer if there is one. Two special cases are not shown in the pseudo code. If all sublists of operations have fewer than k operations, one largest sublist is sent down the tree. Finally, a special and rare case occurs when only groups, consisting solely of copies of deletions, are sent down the tree. If none of these deletions are successful, no operations are removed from the buffer. If this occurs, we empty the whole buffer. Having described all the algorithms needed for maintaining the R R -tree, we proceed to illustrate the workings of the algorithms. 
Example
a b K c d L X f c 1 M h g N Y a 1 e c 2 j a 2 K L a b c d c 1 e f g h M N X Y K L a b c d c 1 e f g h M N X Y a 2 b d c 1 e f g h K M N b K d f c 1 M h g N e c 2 j a 2 a 1 − , a 2  c 1 − , c 2  a − , a 1  , c − a − , a 2  , c − , c 1 − , c 2  j  a − , a 2  , c − , c 1 − c − , c 1 − , c 2  a − , a 2  c − c 1 − , c 2  , j  
a − 1 , a + 2 , c − 1 , c + 2 . 
The deletion of a1 and the corresponding insertion already in the buffer annihilate. The remaining three operations are inserted into the buffer. The insertion of a new object j triggers a buffer emptying. The figure shows how the GROUPOPERATIONS algorithm divides the operations into two groups: one for node X and one for node Y . Note that, due to the overlap between the MBRs of X and Y , c − and c − 1 are copied into both groups. Because the group for node Y has fewer than k elements, its entries are put back into the buffer. The operations in the other group proceed down the tree to nodes K and L, as shown in the figure. Note that, after reading node X, the algorithm discovers that c1 can not be found in this subtree and c − 1 is not sent further, but c − is sent to node L, it succeeds, and its copy is removed from the buffer. The performed operations result in a number of structural changes to the disk tree. Node L is rendered underfull and is merged with node K. As a result, node X contains only one child; thus, X is replaced by its child, reducing the height of the subtree to one. The shorter subtree is then inserted into node Y using the IN- SERTCHILD algorithm. Finally, the single-entry root is removed, finishing the buffer emptying. At last, the operation that caused the buffer emptying, j + , is inserted into the buffer. The resulting disk tree and buffer are shown at the rightmost end of the 
Search Algorithm
The SEARCH algorithm in Algorithm 8 is an adjusted standard R-tree search algorithm. It operates by first querying the disk tree. The resulting answer set is modified to take the buffer data into account. Thus, objects that have corresponding deletions in the buffer are removed from the set, and any objects from the buffer that overlap with the query rectangle are included into the set. As the following theorem shows, the resulting set is the correct answer. THEOREM 2. The SEARCH algorithm, performed after a series of calls to the UPDATE algorithm, returns correct results. PROOF. Assume that the current position of object objId is pc. In a series of update operations concerning object objId , the last operation for any identification tuple objId , p (p = pc) is a deletion . According to Theorem 1, the index is in state 1 or in state 2 with respect to these identification tuples (see 
 The operations are executed by means of an algorithm called EX- ECUTEFROMBUFFER, which is similar to EXECUTEENTRIES used in GROUPUPDATE, except that it takes two additional parameters that specify thresholds for the number of insertions and deletions that can be executed. It also removes the completed operations from the buffer. In Algorithm 9, Smin and Smax is the minimum and the maximum number of allowed entries in an R-tree node, respectively. Because such piggybacking corresponds to the emptying of part of the buffer, the reasoning in the proof of Lemma 2 also applies to show that PIGGYBACK does not violate Property 1. Thus, queryresult correctness is not compromised. Also, note that piggybacking is designed to be a performance optimization that can be disabled without any effects in the correctness of SEARCH or any other algorithm. 
ANALYTICAL COST MODELING
As mentioned in Section 3.6, choosing the right value for the operation threshold (k) is non-trivial. The cost of buffer emptying has to be balanced against the frequency of the buffer emptying. Nevertheless, the emptying of larger groups at the root level should intuitively lead to more sharing of I/O operations. This section describes a cost model that confirms this intuition. More specifically we show that if the buffer size is not very small compared to the total number of objects, the best value for k is the size of the largest group of operations, i.e., only the largest group of operations has to be emptied from the buffer on each buffer emptying. Because the objective of the cost model is not to model the performance accurately in absolute terms, we are able to make a number of simplifying assumptions. We explain these assumptions at the beginning of Sections 4.1 and 4.2. We first model how often the buffer is emptied and then estimate the I/O cost of the buffer emptying. The notation used is summarized in 
Frequency of EmptyBuffer Invocations
 To enable the estimation of the frequency of EMPTYBUFFER invocations , we assume that the number of objects being tracked is O and that Cmax < O. We also assume that the index is in steady state, such that the set of objects being tracked neither grows nor shrinks. A workload then consists of pairs of deletions and insertions . As we assume a general setting where updates are much more frequent than queries, we assume that the effects of piggybacking on the buffer can be ignored. Assume that the an invocation of EMPTYBUFFER has emptied x operations from the buffer, so Cs = Cmax − x operations remain in the buffer. We then proceed to determine how many operations that can be entered into the buffer before the buffer gets full and the next invocation of EMPTYBUFFER occurs. To do that, we need to consider the effects of annihilation because these reduce the size of the buffer. The second case of annihilation, where an incoming insertion annihilates with a deletion in the buffer (the transition from state 2 to state 4 in 
 The first case of annihilation, where an incoming deletion annihilates with an insertion in the buffer, is more frequent. Its probability depends on the number of insertions currently residing in the buffer. Assuming that operations come in deletion-insertion pairs and buffer emptying empties equal amounts of deletions and insertions , half of the operations in the buffer are deletions and half of the operations are insertions, i.e., at any time, there are C/2 insertions in the buffer and the probability of annihilation is C/(2O). To see why this is true, consider what happens when a deletioninsertion pair is added to the buffer. If no annihilation occurs, the number of insertions and the number of deletions in the buffer are each increased by one. If the deletion annihilates with an insertion already in the buffer, the addition of the new insertion results in unchanged numbers of deletions and insertions in the buffer. Summarizing , a deletion-insertion pair leaves C unchanged with probability C/(2O) and increases C by 2 with complementary probability 1 − C/(2O). Between buffer emptyings, the probability of C increasing (Pinc) varies from max(Pinc(C)) = Pinc(Cs) = 1 − Cs/(2O) to min(Pinc(C)) = Pinc(Cmax ) = 1 − Cmax /(2O). Such a process can be modeled precisely as a Markov chain, but, to simplify the analysis, Pinc is assumed to be constant and equal to the average of the minimum and the maximum values: 
Pinc = min(Pinc(C)) + max(Pinc(C)) 2 = 1 − Cs + Cmax 4O 
 We now know the probability with which an incoming deletioninsertion pair will increase the buffer size by two. Assume a workload X of operations consisting of |X|/2 deletion-insertion pairs. Starting with Cs operations in the buffer, and assuming that the buffer does not become full in the process, the expected final number of entries in the buffer after processing the workload is: 
W (X, Cs, Cmax ) = Cs + 2Pinc |X| 2 = Cs + |X|(4O − (Cs + Cmax )) 4O (1) 
 Then the expected buffer-filling workload size |X| can be trivially derived from Equation 1 as follows: 
Cmax = W (X, Cs, Cmax ) ⇒ |X|(Cs, Cmax ) = 4O(Cmax − Cs) 4O − (Cs + Cmax ) 
Expressing |X| as a function of Cmax and x, where x = Cmax − Cs, we obtain: 
|X|(Cmax , x) = 4Ox 4O − 2Cmax + x (2) 
The next section models the I/O cost of buffer emptying. 
EmptyBuffer Cost Model
All update I/O is due to EMPTYBUFFER invocations. In order to estimate the cost of EMPTYBUFFER, we assume that incoming operations are uniformly distributed in the data space; thus, an operation from the buffer is equally likely to be routed by the R-tree algorithms to any of the root's subtrees. Next, we assume that a deletion is only sent to one subtree at the root level. This is a reasonable assumption for R-trees storing point or small-region data (which is the case in our setting) because this results in MBRs with little overlap. We also assume that each object has only one position recorded in the disk tree. Thus, cases such as the one shown for object c in 
Under the above assumptions, we show that emptying only the largest group leads to the smallest amortized cost of update operations . We end by discussing when the last assumption holds. The first step is to compare the amortized I/O cost per update operation resulting from using two different operation threshold values . Thus, we compute the cost when k = kmax , the size of the largest group of operations at the root. Then, we compare this cost with the cost when k = kmax − ( ≥ 0), the size of the next largest group. If K1 is the cost of reading and writing one subtree of the root then the total cost of emptying the largest group from the buffer is 2 + K1 (including the reading and writing of the root node). Dividing this cost by the number of update operations that are necessary to fill the buffer again, we get the amortized update cost when only the largest group is emptied: 
KU,1 = 2 + K1 |X|(Cmax , kmax ) 
Combining this with Equation 2, we get: 
KU,1 = (2 + K1)(4O − 2Cmax + kmax ) 4Okmax (3) 
 If two largest groups are emptied (2kmax − operations in total ), two subtrees of the root are accessed, and the corresponding amortized update cost becomes: 
KU,2 = 2 + 2K1 |X|(Cmax , 2kmax − ) = (2 + 2K1)(4O − 2Cmax + 2kmax − ) 4O(2kmax − ) = (1 + K1)(4O − 2Cmax + 2kmax − ) 4Okmax − 2O (4) 
Comparison of Equations 3 and 4 sheds light on when KU,1 ≤ KU,2. First, note that if the root node was kept in main memory, the first term in the numerators of both formulas would become simply K1, as reading and writing the root would not be necessary. In this case, KU,1 is always smaller than KU,2. We proceed to show that even if the root node is not pinned in memory, KU,1 ≤ KU,2 for sufficiently large Cmax . In particular, the inequality KU,1 ≤ KU,2 can be simplified to: 
(4O − 2Cmax )(2kmax − (2 + K1)) ≤ K1kmax (2kmax − ) (5) 
To see when the inequality holds, we estimate kmax and K1. The cost K1 is equal to twice the average number of nodes in the subtree of the root. Due to the large fan-out of an index tree, the size of the subtree can be accurately approximated by the number of leaf nodes in the subtree, which is, on average, equal to O/(SR), where S is the average fan-out of the disk R-tree nodes (except the root) and R is the current fan-out of the root. Thus, K1 ≈ 2O/(SR). The size of kmax depends on Cmax and R. We assume that the size of the largest group is f times larger than the average group size, i.e., kmax = f Cmax /R, where f > 1. With these estimates of K1 and kmax , it is easy to prove that Equation 5 is satisfied when: 
Cmax ≥ 3SR 2 f . 
This means that for large enough buffers, emptying only the largest group of operations is the most efficient strategy even if the root node of the disk R-tree is not kept in main memory. We proceed to compute the smallest size of the group of operations at the root level, such that emptying this group from the operation buffer results in touching the whole subtree of the root with high probability. This is a key assumption of the analytical model and it provides, indirectly, another lower bound for Cmax . For an entire subtree to be touched when performing a group of operations, all leaves of the subtree must receive at least one operation . In general, distributing n objects among g groups with uniform probability results in groups of objects whose sizes follow the binomial distribution B(n, 1/g) 
f (0; k th , SR/O) = " 1 − SR O « k th 
We require that this probability is low, specifically lower than 0.05: 
f (0; k th , SR/O) ≤ 0.05 ⇒ k th ≥ ln 0.05 ln(1 − SR/O) (6) 
 Using the Taylor series centered at 1 to approximate the denominator , we get k th ≥ 3O/SR. For the cost model to apply, we require that kmax − ≥ k th . Assuming that is small and that kmax = f Cmax /R, this condition can be expressed as follows: 
Cmax ≥ 3O Sf . 
(7) 
Factor f does not have a simple analytical form, but it is a small constant larger than one. In Section 5, we explore experimentally for which settings of Cmax the average size of the largest group of operations satisfies Equation 6. How to choose the optimal value of k for small buffers, when Equation 6 is not satisfied, is an interesting topic for future research. 
EXPERIMENTAL EVALUATION
 In this section we describe the results of performance experiments with the R R -tree and two competing proposals. The settings for the experiments are described first. This is followed by the presentation of results. 
Experimental Setup
Three indexing techniques are covered in the experiments: the R R -tree, the RUM-tree 
Cost Model Validation
The first set of experiments were run with the goal of validating the conclusions of the cost model presented in Section 4, namely, that emptying only the largest group of operations is the most efficient strategy. For this purpose, we compared this strategy with the strategy where threshold k is set to some static value (static thresholding). Uniform and non-uniform workloads were run with different static values of the operation threshold k and two settings for the operation buffer size: 5% and 20% of the number of moving objects. Note that when k = 1, the buffer is emptied completely. Also when, for a specific buffer emptying, k > kmax the algorithm empties just the largest group of operations. 
Exploring the R R -Tree Algorithms
For the rest of the experiments, we use non-uniform workloads. First, we explore selected properties of the R R -tree. The goal of the first experiment is to determine how effective the operation buffer is at increasing the update performance in comparison to the use of an LRU write-back page buffer. In this experiment, part of the available memory is allocated to the operation buffer, and the rest is allocated to the LRU page buffer. The total amount of main memory is kept constant, but the fraction allocated to the operation buffer is changed. The experiment is run with two workloads, one with few queries and one with an equal amount of queries and object updates. The I/O performance is averaged over all three types of operations: insertions, deletions, and queries. 
Comparing the Indexes
To ensure that the comparison of the three indexes fair and thus obtain meaningful results, in each of the experiments, we allocate exactly the same amount of main memory to each index. All the available main memory is used for the page buffer for the R * -tree. The R R -tree uses all of the available memory for the operation buffer in most experiments. The exception is that in the experiments with high ratios of queries, the R R -tree divides memory between the operation buffer and the page buffer. The division used in each case was chosen through a number of performance experiments . The RUM-tree uses a part of the available main memory Buffer size in objects R-tree RUM-tree R R -tree 
RELATED WORK
We proceed to consider relevant previous work on the efficient update of general disk-based data structures and, particularly, on efficient updates in R-trees. Techniques for the bulk-loading of data structures are relevant for the problem of efficient updates. Notably, Choubey et al. 
CONCLUSION AND RESEARCH DIRECTIONS
Motivated by emerging database applications that involve the monitoring of large collections of continuous variables and thus are characterized by high rates of updates, this paper presents a novel data structure, called the R R -tree, that supports updates more efficiently than existing proposals while also supporting queries. In contrast to related work, this data structure is capable of efficiently using any amount of main memory, and it supports the same settings and operations as an ordinary R-tree. Another core difference to the related work is a relaxed persistence assumption: we argue that relaxed persistence of an index is appropriate in a setting with hyper-dynamic data. The R R -tree builds on two main ideas. First, operation buffering in main memory enables execution of the majority of update operations quickly, and it allows some updates to annihilate one another, thus avoiding any I/O at all. Second, by grouping operations on buffer emptying, the index enables all operations that travel to the same node of the tree to share I/O. A strong point of this approach is its orthogonality to the treelike data structure being buffered. This makes it possible to adapt the approach to other types of trees. This yields a quite general contribution . Furthermore, this approach is orthogonal to other means of exploiting the available main memory. This enables adaptation of the R R -tree to different workloads, by combining the operation buffer with some page-cache buffer, e.g., an LRU cache. The empirical performance study includes favorable comparisons with the conventional LRU-cached R-tree and with the state-of-theart RUM-tree. The study also offers general insight into the benefits of LRU caching in the paper's setting. Several interesting directions for future research exist. We expect it to be very interesting to apply the proposed techniques to other indexes in the class of grow-post trees, e.g., the TPR-tree. Next, we note that this paper's focus has been on I/O efficiency. However, the R R -tree does use an in-memory R-tree to efficiently support querying. Thus, the proposal should be competitive with respect to CPU performance. Still, studies of CPU performance and the investigation of techniques that reduce the CPU cost are in order. Finally, while we mention that a simple approach to recover from a main-memory loss is to wait for all the objects to report their positions , other, more advanced, log-based recovery techniques may be invented. 
