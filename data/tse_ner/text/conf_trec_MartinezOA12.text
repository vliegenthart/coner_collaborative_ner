Introduction
In this paper we present the combined submission of the teams NICTA and UBC, which focuses on query expansion techniques. For this edition we build upon the NICTA-2011 systems 
Method
We present here the steps of our approach: we start by describing how we processed the TREC document collection; then we explain our query processing method, including the expansion techniques; finally we detail our indexing and searching approaches. 
Field 
Processing the Document Collection
We apply the same pipeline as in 
Processing Queries
We describe first our methods to identify fields in the query, and then our different expansion approaches. 
Identifying Fields in the Query
We developed a set of manually constructed patterns to map query terms into the available fields (Abbreviation seen in the er|presented to the er REPORT:( " emergency room " OR ER) 
Elderly patients with ventilator-associated pneumonia is translated to: PRESTHIS:(ventilator associated pneumonia) OR DISCHDIAG:(ventilator associated pneumonia) OR AGE:(age60 age70 age80 age90) OR REPORT:(elderly with ventilator associated pneumonia). A small number of abbreviations, such as ER (emergency room), were also expanded in the queries. 
Query Expansion using Semantic Types (ST)
We leveraged external resources to add new terms to our queries, by identifying terms that are strongly related to the query terms. Specifically, we focused on query terms that represent medical categorical concepts (e.g. disease categories). For example, for the query below, we added terms falling under the category of " atypical antipsychotics " : 
Patients taking atypical antipsychotics 
Our approach to expansion used two main knowledge sources: the UMLS Metathesaurus (version 2010aa) and DBpedia. In order to select expansion candidates we used MetaMap- 2010 from the National Library of Medicine (NLM). We defined manual expansion rules from these resources based on the sample queries of TREC-2011 and 50 queries from the priority list from the US Institute of Medicine of the National Academies 3 . Using these queries, we defined a small set of stop-categories that would have otherwise produced undesirable expansions. The following terms were excluded from expansion: " administration " , " AMA " , " diagnosis " , " drug " , " functional concept " , " medication " , and " surgery " . We also removed terms with the following strings from the DBpedia output: " code " , " history " , " mechanism " , " poisoning " , " toxicity " , and " withdrawal " . During the development process, we explored expansion using hierarchical relations from the UMLS Metathesaurus, by selecting all the terms in the hyponym concepts; however, we observed that DBpedia offered a higher coverage of some domains, such as newly developed drugs, and it also showed less risk of over-expansion. For instance, one sample query contained the term " atypical antipshychotic " , which UMLS expanded with 8 more specific drugs (e.g. " Clozapine " ). DBpedia, however, identified the same set of drugs, as well as a further 22 new drug and brand names, which seemed correct after manual analysis, and had a stronger presence in the collection. For our final expansion system, we first applied MetaMap to identify phrases linked to terms in the UMLS Metathesaurus. The matched concepts were then used as candidate terms to be expanded; in some cases terms consisted of a primary term followed by a parenthesized description — such as " Intervention (Surgical and medical procedures) " — and in such cases we treated them as separate candidate terms. Each candidate term had a Semantic Type (ST) associated with it in the MetaMap output. We used STs to define two expansion groups: safe expansion (for terms which STs include the string " Pharmacologic Substance " ) and filtered expansion (for terms whose ST is " Therapeutic or Preventive Procedure " ). Candidate terms that did not belong to these groups were discarded; for the rest, if they were listed as " category " in DBpedia 4 , we extracted all of the terms listed under the category as our expansion terms. Then, for " safe expansion " the output was the full list of expansion terms; for " filtered expansion " , we removed terms which are not UMLS concepts by applying MetaMap to each term. 
Query Expansion using Personalised PageRank
 For this approach, we use a graph algorithm based on random walks over the graph representation of a knowledge-base of concepts and relations, to obtain concepts related to the queries. The UMLS Metathesaurus is used as the knowledge-base, and we represent UMLS as a graph. Apart from concepts, UMLS Metathesaurus also contains a wide range of information about the relations between concepts in the form of database tables. The MRREL table lists relations between concepts like " parent " , " can be qualified by " or " related and possibly synonymous " among others. The MRCOC table contains co-occurrence relations between concepts, that is, relations between similar concepts or different concepts that share an important connection. In order to obtain the graph structure of UMLS, we simply treat the concepts in UMLS as vertices, and the relations listed in the MRREL and MRCOC tables as edges. No weights are used for the relations that are extracted from the MRREL table. Given a query and the graph-based representation of UMLS, we obtain a ranked list of related concepts as follows: 1. We first run MetaMap and identify the UMLS concepts in the query, we explore two variants: with and without the in-built Word Sense Disambiguation (WSD) module. We also rely on the NegEx module to remove negated concepts. Note that in cases where we rely on field search, we treat each field as a separate query for this kind of expansion. 
2. We then assign a uniform probability distribution to the concepts found in the query. The rest of nodes are initialized to zero. 3. We compute personalized PageRank 
Basically, personalized PageRank is computed by modifying the random jump distribution vector in the traditional PageRank equation. In our case, we concentrate all probability mass in the concepts identified in the query. Let G be a graph with N vertices v 1 , . . . , v N and d i be the outdegree of node i; let M be a N × N transition probability matrix, where M ji = 1 d i if a link from i to j exists, and zero otherwise. Then, the calculation of the PageRank vector Pr over G is equivalent to resolving Equation (1). 
Pr = cM Pr + (1 − c)v (1) 
In the equation, v is a N × 1 vector and c is the so-called damping factor, a scalar value between 0 and 1. The first term of the sum on the equation models the voting scheme described in the beginning of the section. The second term represents, loosely speaking, the probability of a surfer randomly jumping to any node, e.g. without following any paths on the graph. The damping factor, usually set in the 
Combined Query Expansion
In order to combine our two different expansion techniques, we can simply merge the terms from each expansion source into a joint query. Another approach that we explored is to rely on the expanded terms from the ST-expansion to initialise the PageRank method. We report the results of the two methods in our experiments. 
Indexing and Searching
We first distinguish between two types of indexing in our runs: visit-based and report-based. In the former approach, all related reports for a visit were concatenated (removing duplicate diagnostics codes) to create a single " multi-document " item for indexing. We refer to the former approach as VISIT, and as REPORT to the latter. As explained in Section 2.1, we also generate different indexes depending on the use of separate fields or not (FIELDS/COMBINED), or the application of stemming (STEM/NOSTEM). When we rely on field search, a Boolean search over the fields is followed by ranking. We used stop-word removal both in query processing and indexing; however, we augmented the typical list of stop-words with patient, and removed single characters, and, or, not, and no from the list. Regarding negation, as explained in Section 2.1, we pre-processed the document collection with NegEx, in order to to handle negated terms, and built separate indices. Since we only observed minor differences, we settled on a single index for each of the collections. Thus, we report here the results using the NegEx-processed index for TREC-2011, and the full index for TREC-2012. The search engine used for indexing and searching in our runs was Apache Lucene (v3.2); we used both the BM25 and tf-idf ranking algorithms for Lucene 
Results over the TREC-2011 query set
We first tested different combinations of our main approaches over the TREC-2011 query set and collection, in order to select the most promising configurations for TREC-2012. We relied on the same evaluation metric that was used in TREC-2011: Bpref. We performed three main experiments: As mentioned above, when we combine PageRank and ST, we have to choose if we want to apply PageRank over the query concepts, or over the ST-expanded concept set. We present the results for the two different settings in most of our experiments. There are other two alternatives when applying PageRank: to perform WSD prior to choosing the initial concepts, or not to use WSD. We report here the results of the two variants. Finally, we also need to set a threshold to decide the number of top concepts to use. As mentioned above, we performed preliminary experiments using two types of thresholds: weight-based (i.e. choose all the concepts above the cut-off PageRank weight) and ranking-based (i.e. select all the concepts in the top k positions), and settled on the latter setting. We report the results for the best and worst cut-offs in the range 3-20 over the TREC-2011 dataset. We start our analysis by evaluating the performance of PageRank without ST expansions. In this case we also need to decide whether we parse the query before applying PageRank or not. For our first experiment we chose the index VISIT+STEM+COMBINED and TF-IDF ranking as basic system, since it achieved the highest performance in previous experiments when no ST expansions were used. The results over the TREC-2011 query set are given in 
System 
System 
 We observe that PageRank expansion helps to improve the baseline, and that the best performance is obtained when combining it with ST, even if ST alone does not perform well. 
Official results over the TREC-2012 query set
At the time of submitting the runs, we did not have all the information regarding the optimal values of combinations and parameters, so we chose four configurations that had achieved good performance over the TREC-2011 dataset at the time. For all our runs, we relied on the COMBINED index (since FIELDS did not perform well over TREC-2011), and we did not process negations for the documents (only for the queries), we also use TF-IDF in all the submitted runs: @BULLET NICTAUBC1: Combined expansion, PageRank first (threshold = 3), index REPORT+STEM @BULLET NICTAUBC2: Combined expansion, ST expansion first (threshold = 4), index REPORT+NOSTEM @BULLET NICTAUBC4: ST expansion, index VISIT+STEM @BULLET NICTAUBC6: Combined expansion, ST expansion first (threshold = 6), index VISIT+NOSTEM The results of the different systems are given in 
Additional experiments
After the qrels were released, we already had obtained the complete set of results on TREC 2011, and we performed additional experiments. We first checked the performance of the best 2011 configurations (cf. 
Expansion 
Conclusions
This year We have tested two different methods for query expansion based on DbPedia and UMLS. The first method is heuristic query expansion, and the second is based on random walks over UMLS. Our development experiments on TREC-2011 showed that our heuristic and random-walk expansion algorithms (ST and PageRank, respectively) where very successful , with PageRank providing better results and the combination beating the best reported results. When submitting the runs to TREC-2012 our development experiments where not completely finished. Our best run was based on ST expansion alone, and ranked 11th out of the 82 automatic runs. When development finished we were able to improve the PageRank results, and show that both PageRank and ST were improving performance over our baseline system. The best results were those of ST expansion, as submitted to the official task. In addition, we also report the results when optimizing parameters on the 2012 dataset and evaluating on 2011. The results confirm that both expansion strategies overcome the baseline, with PageRank performing better than ST, and the combination providing the best results. In the future, we plan to perform a thorough analysis of the different queries, in order to learn the reasons for the discrepancy between 2011 and 2012 dataset, and to explore better ways to develop expansion techniques that benefit from the combined expansion approach over medical data. 
